#!/usr/bin/env python3

# generate learning curves, confusion matrices, precision-recall curves, thresholds, etc.
 
# e.g. accuracy \
#     --logdir=trained-classifier \
#     --loss=exclusive \
#     --overlapped_prefix=not_ \
#     --error_ratios=2,1,0.5 \
#     --nprobabilities=50 \
#     --parallelize=-1

import argparse
import sys
import os
import re
import numpy as np
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
#plt.ion()
import csv
from natsort import natsorted, index_natsorted
import matplotlib.cm as cm
from datetime import datetime
import socket
from itertools import cycle, islice

from lib import *

srcdir, repodir, _ = get_srcrepobindirs()

from scipy import interpolate
from scipy import stats
import math
  
FLAGS = None

def plot_metrics_parameterized_by_threshold(thresholds, \
                                            metric1_data, metric2_data, \
                                            metric1_label, metric2_label, \
                                            areas, labels, probabilities, ratios):
  speciess = sorted(list(set([x.split('-')[0] for x in labels])))
  if len(speciess)<len(labels):
    ncols = len(speciess)
    nrows = max([sum([x.split('-')[0]==s for x in labels]) for s in speciess])
    colcounts = np.zeros(ncols, dtype=np.uint8)
  else:
    nrows, ncols = layout(len(labels))
  fig=plt.figure(figsize=(3.2*ncols, 2.4*nrows))
  for ilabel in range(len(labels)):
    if len(speciess)<len(labels):
      icol = np.argmax([labels[ilabel].split('-')[0]==s for s in speciess])
      irow = colcounts[icol]
      colcounts[icol] += 1
      ax=plt.subplot(nrows,ncols,irow*ncols+icol+1)
    else:
      ax=plt.subplot(nrows,ncols,ilabel+1)
    if ilabel==0:
      ax.set_xlabel(metric2_label)
      ax.set_ylabel(metric1_label)
    ax.set_title(labels[ilabel]+',  area='+str(np.round(areas[labels[ilabel]],3)))
    ax.set_xlim(0,1)
    ax.set_ylim(0,1)
    line, = plt.plot(metric2_data[labels[ilabel]], metric1_data[labels[ilabel]])
    for (ithreshold,threshold) in enumerate(thresholds[labels[ilabel]]):
      if np.isnan(threshold):
        continue
      iprobability = np.argmin(np.abs(probabilities[labels[ilabel]]-threshold))
      if ratios[ithreshold]==1:
        plt.plot(metric2_data[labels[ilabel]][iprobability], \
                 metric1_data[labels[ilabel]][iprobability], 'ro')
      else:
        plt.plot(metric2_data[labels[ilabel]][iprobability], \
                 metric1_data[labels[ilabel]][iprobability], 'rx')
      whichside = 'right' if metric2_data[labels[ilabel]][iprobability]>0.5 else 'left'
      ax.annotate(str(np.round(threshold,3))+' ',
                  xy=(metric2_data[labels[ilabel]][iprobability], \
                      metric1_data[labels[ilabel]][iprobability]),
                  color="r", verticalalignment='top', horizontalalignment=whichside)
  fig.tight_layout()

def plot_probability_density(test_ground_truth, test_logits, ratios, thresholds, labels, loss):
  speciess = sorted(list(set([x.split('-')[0] for x in labels])))
  if len(speciess)<len(labels):
    ncols = len(speciess)
    nrows = max([sum([x.split('-')[0]==s for x in labels]) for s in speciess])
    colcounts = np.zeros(ncols, dtype=np.uint8)
  else:
    nrows, ncols = layout(len(labels))
  x = np.arange(0, 1.01, 0.01)
  fig=plt.figure(figsize=(3.2*ncols, 2.4*nrows))
  for ilabel in range(len(labels)):
    if len(speciess)<len(labels):
      icol = np.argmax([labels[ilabel].split('-')[0]==s for s in speciess])
      irow = colcounts[icol]
      colcounts[icol] += 1
      ax=plt.subplot(nrows,ncols,irow*ncols+icol+1)
    else:
      ax=plt.subplot(nrows,ncols,ilabel+1)
    if ilabel==0:
      ax.set_xlabel('Probability')
      ax.set_ylabel('Density')
    ax.set_title(labels[ilabel])
    for (ithreshold,threshold) in enumerate(thresholds[labels[ilabel]]):
      if np.isnan(threshold):
        continue
      if ratios[ithreshold]==1:
        plt.axvline(x=threshold, color='k', linestyle='dashed')
      else:
        plt.axvline(x=threshold, color='k', linestyle='dotted')
    for gt in range(len(labels)):
      if loss=='exclusive':
        igt = test_ground_truth==gt
      else:
        igt = test_ground_truth[:,ilabel]==gt
      if sum(igt)==0:
        continue
      xdata = test_logits[igt,ilabel]
      xdata = np.minimum(np.finfo(float).max, np.exp(xdata))
      xdata = xdata / (xdata + 1)
      if len(xdata)<2 or any(np.isnan(xdata)):
        continue
      density = stats.gaussian_kde(xdata)
      y = density(x)
      line, = plt.plot(x, y)
      if ilabel==gt:
        ax.set_ylim(0,max(y))
      if ilabel==0:
        line.set_label(labels[gt])
  fig.tight_layout()
  return fig.legend(loc='lower left')

def doit(logdir, key_to_plot, ckpt, labels, nprobabilities, error_ratios, loss,
        overlapped_prefix):
  validation_sounds, validation_ground_truth, validation_logits = \
        read_logits(logdir, key_to_plot, ckpt)

  probabilities, thresholds, precisions, recalls, sensitivities, specificities, \
        pr_areas, roc_areas = calculate_precision_recall_specificity( \
        validation_ground_truth, validation_logits, labels, nprobabilities, \
        error_ratios, loss)

  save_thresholds(logdir, key_to_plot, ckpt, thresholds, error_ratios, labels)

  plot_metrics_parameterized_by_threshold(thresholds, precisions, recalls, \
        'precision = Tp/(Tp+Fp)', 'recall = Tp/(Tp+Fn)', pr_areas, \
        labels, probabilities, error_ratios)
  plt.savefig(os.path.join(logdir,key_to_plot,'precision-recall.ckpt-'+str(ckpt)+'.pdf'))
  plt.close()

  plot_metrics_parameterized_by_threshold(thresholds, specificities, \
        sensitivities, 'specificity = Tn/(Tn+Fp)', 'sensitivity = Tp/(Tp+Fn)', \
        roc_areas, labels, probabilities, error_ratios)
  plt.savefig(os.path.join(logdir, key_to_plot, \
                           'specificity-sensitivity.ckpt-'+str(ckpt)+'.pdf'))
  plt.close()

  already_written = set()
  predictions_path = os.path.join(logdir,key_to_plot,'predictions.ckpt-'+str(ckpt))
  if not os.path.isdir(predictions_path):
    os.mkdir(os.path.join(logdir,key_to_plot,'predictions.ckpt-'+str(ckpt)))
  if loss=='exclusive':
      for subdir in set([x['file'][0] for x in validation_sounds]):
          with open(os.path.join(logdir, key_to_plot, 'predictions.ckpt-'+str(ckpt), \
                                 subdir+'-mistakes.csv'), \
                    'w', newline='') as csvfile:
            csvwriter = csv.writer(csvfile, lineterminator='\n')
            for i in range(len(validation_sounds)):
                if validation_sounds[i]['file'][0] != subdir:
                    continue
                classified_as = np.argmax(validation_logits[i,:])
                id = os.path.join(*validation_sounds[i]['file']) + \
                     str(validation_sounds[i]['ticks']) + \
                     validation_sounds[i]['label'] + \
                     labels[classified_as]
                if id in already_written:
                    continue
                already_written |= set([id])
                csvwriter.writerow([validation_sounds[i]['file'][1],
                        validation_sounds[i]['ticks'][0],
                        validation_sounds[i]['ticks'][1],
                        'correct' if classified_as == validation_ground_truth[i] else 'mistaken',
                        labels[classified_as],
                        validation_sounds[i]['label']])
  else:
      for subdir in set([x[0]['file'][0] for x in validation_sounds]):
          with open(os.path.join(logdir, key_to_plot, 'predictions.ckpt-'+str(ckpt), \
                                 subdir+'-mistakes.csv'), \
                    'w', newline='') as csvfile:
            csvwriter = csv.writer(csvfile, lineterminator='\n')
            for i in range(len(validation_sounds)):
                if validation_sounds[i][0]['file'][0] != subdir:
                    continue
                for j in range(len(validation_sounds[i])):
                    k = labels.index(validation_sounds[i][j]['label'].removeprefix(overlapped_prefix))
                    classified_as = validation_logits[i,k]>0.5
                    id = os.path.join(*validation_sounds[i][j]['file']) + \
                         str(validation_sounds[i][j]['ticks']) + \
                         validation_sounds[i][j]['label'] + \
                         str(classified_as)
                    if id in already_written:
                        continue
                    already_written |= set([id])
                    csvwriter.writerow([validation_sounds[i][j]['file'][1],
                            validation_sounds[i][j]['ticks'][0],
                            validation_sounds[i][j]['ticks'][1],
                            'correct' if classified_as == validation_ground_truth[i][j] else 'mistaken',
                            labels[k] if classified_as==1 else overlapped_prefix+labels[k],
                            validation_sounds[i][j]['label']])

  lgd = plot_probability_density(validation_ground_truth, validation_logits, \
                                 error_ratios, thresholds, labels, loss)
  plt.savefig(os.path.join(logdir, key_to_plot, \
                           'probability-density.ckpt-'+str(ckpt)+'.pdf'), \
              bbox_extra_artists=(lgd,), bbox_inches='tight')

  plt.close()
  return 1

def main():
    flags = vars(FLAGS)
    for key in sorted(flags.keys()):
      print('%s = %s' % (key, flags[key]))

    error_ratios = [float(x) for x in FLAGS.error_ratios.split(',')]
  
    train_accuracy, train_loss, train_time, train_step, \
          validation_precision, validation_recall, \
          validation_precision_mean, validation_recall_mean, \
          validation_time, validation_step, \
          _, _, _, _, \
          labels_touse, label_counts, _, _, batch_size, _ = \
          read_logs(FLAGS.logdir)
    training_set_size = {k: len(label_counts[k]["training"]) * \
                            np.max(list(label_counts[k]["training"].values())) \
                         for k in label_counts.keys()}
  
    keys_to_plot = natsorted(train_step.keys())
  
    nrows, ncols = layout(len(keys_to_plot))
    fig = plt.figure(figsize=(6.4*ncols, 4.8*nrows))
    fig.subplots_adjust(bottom=0.2)
    for (imodel,model) in enumerate(keys_to_plot):
      ax4 = fig.add_subplot(nrows, ncols, imodel+1)
      nx = len(train_step[model])
      idx = range(nx) if nx<10000 else np.round(np.linspace(0, nx-1, 10000)).astype(int)
      ax4.plot(np.array(train_step[model])[idx],
               np.array(train_loss[model])[idx],
               'g', alpha=0.5, label='Loss')
      ax4.set_xlabel('Step')
      ax4.set_ylabel('Loss', color='g')
      ax4.tick_params(axis='y', labelcolor='g')
      ax1 = ax4.twinx()
      ax1.plot(np.array(train_step[model])[idx],
               np.array(train_accuracy[model])[idx],
               'b', alpha=0.5, label='Train')
      validation_intervals = list(zip(validation_step[model][0:-1], \
                                      validation_step[model][1:]))
      train_accuracy_ave = [np.mean(train_accuracy[model][train_step[model].index(x): \
                                                          train_step[model].index(y)]) \
                            for (x,y) in validation_intervals]
      ax1.plot([(x+y)/2 for (x,y) in validation_intervals], train_accuracy_ave, 'c', label='Train mean')
      train_loss_ave = [np.mean(train_loss[model][train_step[model].index(x): \
                                                  train_step[model].index(y)]) \
                            for (x,y) in validation_intervals]
      ax4.plot([(x+y)/2 for (x,y) in validation_intervals], train_loss_ave, 'm', label='Loss mean')
      if validation_recall_mean[model]:
        ax1.plot(validation_step[model], validation_recall_mean[model], 'r', label='Validation')
        ax1.set_title(model+"   "+str(round(max(validation_recall_mean[model]),1))+'%')
        ax1.set_ylim(bottom=min(validation_recall_mean[model]))
      ax1.set_ylim(top=100)
      ax1.set_ylabel('Overall recall')
      ax1.set_xlim(0,1+len(train_step[model]))
      if imodel==len(keys_to_plot)-1:
        handles1, labels1 = ax1.get_legend_handles_labels()
        handles4, labels4 = ax4.get_legend_handles_labels()
        ax1.legend(handles1+handles4, labels1+labels4, loc=(1.25, 0.0)) #loc='upper center')
      ax2 = ax4.twiny()
      ax2.xaxis.set_ticks_position("bottom")
      ax2.xaxis.set_label_position("bottom")
      ax2.spines["bottom"].set_position(("axes", -0.2))
      ax2.set_frame_on(True)
      ax2.patch.set_visible(False)
      for k in ax2.spines.keys():
          ax2.spines[k].set_visible(False)
      ax2.spines["bottom"].set_visible(True)
      scaled_data,units = choose_units(train_time[model])
      ax2.set_xlim(scaled_data[0],scaled_data[-1])
      ax2.set_xlabel('Time ('+units+')')
      ax3 = ax4.twiny()
      ax3.xaxis.set_ticks_position("bottom")
      ax3.xaxis.set_label_position("bottom")
      ax3.spines["bottom"].set_position(("axes", -0.4))
      ax3.set_frame_on(True)
      ax3.patch.set_visible(False)
      for k in ax3.spines.keys():
          ax3.spines[k].set_visible(False)
      ax3.spines["bottom"].set_visible(True)
      step2epoch = batch_size[model] / training_set_size[model]
      ax3.set_xlim(train_step[model][0]*step2epoch, \
                   train_step[model][-1]*step2epoch)
      ax3.set_xlabel('Epoch')
  
    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'train-validation-loss.pdf'))
    plt.close()
  

    def PvR(ax, precision, recall, validation_step, minp, minr):
        minp = min(minp, min(precision))
        minr = min(minr, min(recall))
        ax.set_prop_cycle(color=[cm.viridis(1.*i/max(1,len(recall)-2)) \
                                         for i in range(len(recall)-1)])
        for i in range(len(recall)-1):
          ax.plot(recall[i:i+2], precision[i:i+2])
        ax.annotate(str(validation_step[0]),
                    xy=(recall[0], precision[0]),
                    color="r", verticalalignment='center', horizontalalignment='center')
        ax.annotate(str(validation_step[-1]),
                    xy=(recall[-1], precision[-1]),
                    color="r", verticalalignment='center', horizontalalignment='center')
        ibestF1 = np.argmax([2*p*r/(p+r) if (p+r)>0 else np.nan \
                             for (p,r) in zip(precision,recall)])
        ax.annotate(str(validation_step[ibestF1]),
                    xy=(recall[ibestF1], precision[ibestF1]),
                    color="r", verticalalignment='center', horizontalalignment='center')
        ax.yaxis.tick_right()
        return minp, minr

    these_labels_touse = labels_touse[keys_to_plot[0]]

    nrows, ncols = 1+len(these_labels_touse), 1+len(keys_to_plot)
    fig = plt.figure(figsize=(6.4*ncols, 4.8*nrows))
    minp, minr = 100, 100
    axs = []
    precision_all, recall_all = [], []
    ilabel_all, imodel_all = [], []
    for (ilabel,label) in enumerate(these_labels_touse):
      for (imodel,model) in enumerate(keys_to_plot):
        axs.append(fig.add_subplot(nrows, ncols, (ilabel+1)*ncols+imodel+2))
        precision = [100*x[ilabel] for x in validation_precision[model]]
        recall = [100*x[ilabel] for x in validation_recall[model]]
        precision_all.append(precision)
        recall_all.append(recall)
        ilabel_all.append(ilabel)
        imodel_all.append(imodel)
        minp, minr = PvR(axs[-1], precision, recall, validation_step[model], minp, minr)
        if imodel==len(keys_to_plot)-1:
            axs[-1].yaxis.set_label_position("right")
            axs[-1].set_ylabel("Precision")
        if ilabel==len(these_labels_touse)-1:
            axs[-1].set_xlabel("Recall")
    for (imodel,model) in enumerate(keys_to_plot):
        axs.append(fig.add_subplot(nrows, ncols, imodel+2))
        precision = np.nanmean([precision_all[i] for (i,idx) in enumerate(imodel_all) if idx==imodel],
                            axis=0)
        recall = np.nanmean([recall_all[i] for (i,idx) in enumerate(imodel_all) if idx==imodel],
                         axis=0)
        minp, minr = PvR(axs[-1], precision, recall, validation_step[model], minp, minr)
        axs[-1].set_title(model)
        if imodel==len(keys_to_plot)-1:
            axs[-1].yaxis.set_label_position("right")
            axs[-1].set_ylabel("Precision")
    for (ilabel,label) in enumerate(these_labels_touse):
        axs.append(fig.add_subplot(nrows, ncols, (ilabel+1)*ncols+1))
        precision = np.nanmean([precision_all[i] for (i,idx) in enumerate(ilabel_all) if idx==ilabel],
                            axis=0)
        recall = np.nanmean([recall_all[i] for (i,idx) in enumerate(ilabel_all) if idx==ilabel],
                         axis=0)
        minp, minr = PvR(axs[-1], precision, recall, validation_step[model], minp, minr)
        axs[-1].set_ylabel(label)
        if ilabel==len(these_labels_touse)-1:
            axs[-1].set_xlabel("Recall")
    axs.append(fig.add_subplot(nrows, ncols, 1))
    precision = np.nanmean(precision_all, axis=0)
    recall = np.nanmean(recall_all, axis=0)
    minp, minr = PvR(axs[-1], precision, recall, validation_step[model], minp, minr)
    axs[-1].set_ylabel('average')
    axs[-1].set_title('average')
    for ax in axs:
      ax.set_ylim([minp,100])
      ax.set_xlim([minr,100])
    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'PvR.pdf'))
    plt.close()
  
  
    def epoch_axis(ax, batch_size, training_set_size, validation_step):
        ax3 = ax.twiny()
        ax3.xaxis.set_ticks_position("bottom")
        ax3.xaxis.set_label_position("bottom")
        ax3.spines["bottom"].set_position(("axes", -0.15))
        ax3.set_frame_on(True)
        ax3.patch.set_visible(False)
        for k in ax3.spines.keys():
            ax3.spines[k].set_visible(False)
        ax3.spines["bottom"].set_visible(True)
        step2epoch = batch_size / training_set_size
        ax3.set_xlim(validation_step[0]*step2epoch, validation_step[-1]*step2epoch)
        ax3.set_xlabel('Epoch')


    nrows, ncols = 3, len(these_labels_touse)
    fig = plt.figure(figsize=(6.4*ncols, 4.8*nrows))
    lines=[]
    isort = index_natsorted(these_labels_touse)
    for ilabel in range(len(isort)):
      ax_p = fig.add_subplot(nrows, ncols, ilabel+1)
      ax_r = fig.add_subplot(nrows, ncols, ncols+ilabel+1)
      ax_f1 = fig.add_subplot(nrows, ncols, 2*ncols+ilabel+1)
      P_all, R_all, F1_all = [], [], []
      for (imodel,model) in enumerate(keys_to_plot):
        P_all.append([100*p[isort[ilabel]] for p in validation_precision[model]])
        R_all.append([100*r[isort[ilabel]] for r in validation_recall[model]])
        F1_all.append([2*p*r/(p+r) if (p+r)>0 else np.nan for (p,r) in zip(P_all[-1],R_all[-1])])
        ax_p.plot(validation_step[model], P_all[-1])
        ax_r.plot(validation_step[model], R_all[-1])
        line, = ax_f1.plot(validation_step[model], F1_all[-1])
        if ilabel==0:
          lines.append(line)
      ax_p.set_ylim(top=100)
      ax_r.set_ylim(top=100)
      ax_f1.set_ylim(top=100)
      ax_f1.set_xlabel('Step')
      if ilabel==0:
        ax_p.set_ylabel('Precision')
        ax_r.set_ylabel('Recall')
        ax_f1.set_ylabel('F1 = 2PR/(P+R)')
      ax_p.set_title(these_labels_touse[isort[ilabel]])
      ax_p.plot(validation_step[model], np.nanmean(P_all, axis=0), color='k', linewidth=3)
      ax_r.plot(validation_step[model], np.nanmean(R_all, axis=0), color='k', linewidth=3)
      line, = ax_f1.plot(validation_step[model], np.nanmean(F1_all, axis=0), color='k', linewidth=3)
      if ilabel==0:
        lines.append(line)
      epoch_axis(ax_f1, batch_size[model], training_set_size[model], validation_step[model])
    lgd = fig.legend(lines, [*keys_to_plot, 'mean'], bbox_to_anchor=(1,1), loc="upper left")
    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'P-R-F1-label.pdf'),
                bbox_extra_artists=(lgd,), bbox_inches='tight')
    plt.close()


    isort = index_natsorted(these_labels_touse)
  
    nrows, ncols = 3, len(keys_to_plot)
    fig = plt.figure(figsize=(6.4*ncols, 4.8*nrows))
    lines=[]
    for (imodel,model) in enumerate(keys_to_plot):
      ax_p = fig.add_subplot(nrows, ncols, imodel+1)
      ax_r = fig.add_subplot(nrows, ncols, ncols+imodel+1)
      ax_f1 = fig.add_subplot(nrows, ncols, 2*ncols+imodel+1)
      P_all, R_all, F1_all = [], [], []
      for ilabel in range(len(isort)):
        P_all.append([100*p[isort[ilabel]] for p in validation_precision[model]])
        R_all.append([100*r[isort[ilabel]] for r in validation_recall[model]])
        F1_all.append([2*p*r/(p+r) if (p+r)>0 else np.nan for (p,r) in zip(P_all[-1],R_all[-1])])
        ax_p.plot(validation_step[model], P_all[-1])
        ax_r.plot(validation_step[model], R_all[-1])
        line, = ax_f1.plot(validation_step[model], F1_all[-1])
        if imodel==0:
          lines.append(line)
      ax_p.set_ylim(top=100)
      ax_r.set_ylim(top=100)
      ax_f1.set_ylim(top=100)
      ax_f1.set_xlabel('Step')
      if imodel==0:
        ax_p.set_ylabel('Precision')
        ax_r.set_ylabel('Recall')
        ax_f1.set_ylabel('F1 = 2PR/(P+R)')
      ax_p.set_title(model)
      ax_p.plot(validation_step[model], np.nanmean(P_all, axis=0), color='k', linewidth=3)
      ax_r.plot(validation_step[model], np.nanmean(R_all, axis=0), color='k', linewidth=3)
      line, = ax_f1.plot(validation_step[model], np.nanmean(F1_all, axis=0), color='k', linewidth=3)
      if imodel==0:
        lines.append(line)
      epoch_axis(ax_f1, batch_size[model], training_set_size[model], validation_step[model])
    theselabels = [these_labels_touse[i] for i in isort]
    lgd = fig.legend(lines, [*theselabels, 'mean'], bbox_to_anchor=(1,1), loc="upper left")
    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'P-R-F1-model.pdf'),
                bbox_extra_artists=(lgd,), bbox_inches='tight')
    plt.close()

  
    nrows, ncols = 1, 3
    fig = plt.figure(figsize=(6.4*ncols, 4.8*nrows))
    ax_p = fig.add_subplot(nrows, ncols, 1)
    ax_r = fig.add_subplot(nrows, ncols, 2)
    ax_f1 = fig.add_subplot(nrows, ncols, 3)
    P_all, R_all, F1_all = [], [], []
    errorbars = len(keys_to_plot) * len(isort) > 10
    for (imodel,model) in enumerate(keys_to_plot):
      for ilabel in range(len(isort)):
        P_all.append([100*p[isort[ilabel]] for p in validation_precision[model]])
        R_all.append([100*r[isort[ilabel]] for r in validation_recall[model]])
        F1_all.append([2*p*r/(p+r) if (p+r)>0 else np.nan for (p,r) in zip(P_all[-1],R_all[-1])])
        if not errorbars:
            ax_p.plot(validation_step[model], P_all[-1], color='grey')
            ax_r.plot(validation_step[model], R_all[-1], color='grey')
            ax_f1.plot(validation_step[model], F1_all[-1], color='grey')
    ax_p.set_ylim(top=100)
    ax_r.set_ylim(top=100)
    ax_f1.set_ylim(top=100)
    ax_p.set_xlabel('Step')
    ax_r.set_xlabel('Step')
    ax_f1.set_xlabel('Step')
    ax_p.set_ylabel('Precision')
    ax_r.set_ylabel('Recall')
    ax_f1.set_ylabel('F1 = 2PR/(P+R)')
    if errorbars:
        y, e = np.nanmean(P_all, axis=0), np.nanstd(P_all, axis=0)
        ax_p.errorbar(validation_step[model], y, e, color='k')
        ax_p.set_ylim(bottom=min(y-e))
        y, e = np.nanmean(R_all, axis=0), np.nanstd(R_all, axis=0)
        ax_r.errorbar(validation_step[model], y, e, color='k')
        ax_r.set_ylim(bottom=min(y-e))
        y, e = np.nanmean(F1_all, axis=0), np.nanstd(F1_all, axis=0),
        ax_f1.errorbar(validation_step[model], y, e, color='k')
        ax_f1.set_ylim(bottom=min(y-e))
    else:
        ax_p.plot(validation_step[model], np.nanmean(P_all, axis=0), color='k', linewidth=3)
        ax_r.plot(validation_step[model], np.nanmean(R_all, axis=0), color='k', linewidth=3)
        ax_f1.plot(validation_step[model], np.nanmean(F1_all, axis=0), color='k', linewidth=3)
    epoch_axis(ax_p, batch_size[model], training_set_size[model], validation_step[model])
    epoch_axis(ax_r, batch_size[model], training_set_size[model], validation_step[model])
    epoch_axis(ax_f1, batch_size[model], training_set_size[model], validation_step[model])
    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'P-R-F1-average.pdf'))
    plt.close()
    print('ckpts_all =', validation_step[model])
    print('precisions_all =', P_all)
    print('recalls_all =', R_all)

  
    if not len(validation_recall_mean[keys_to_plot[0]]):
        sys.exit()
  
    confusion_matrices, labels = \
        parse_confusion_matrices(FLAGS.logdir, next(iter(keys_to_plot)).split('_')[0])
  
    recall_matrices={}
    precision_matrices={}
    precisions_mean={}
    recalls_mean={}
    best_F1={}
    for model in keys_to_plot:
        recall_matrices[model]={}
        precision_matrices[model]={}
        precisions_mean[model]={}
        recalls_mean[model]={}
        best_F1[model]=("",0)
        for ckpt in confusion_matrices[model].keys():
            precision_matrices[model][ckpt]=[None]*len(confusion_matrices[model][ckpt])
            recall_matrices[model][ckpt]=[None]*len(confusion_matrices[model][ckpt])
            if FLAGS.loss=='exclusive':
                nrows, ncols = 1,1
                fig = plt.figure(figsize=(6.4, 4.8))
            else:
                nrows, ncols = layout(1+len(labels))
                fig = plt.figure(figsize=(4.8*ncols, 3.6*nrows))
            for iconfmat, confusion_matrix in enumerate(confusion_matrices[model][ckpt]):
                ax = plt.subplot(nrows, ncols, 1 if FLAGS.loss=='exclusive' else 2+iconfmat)
                precision_matrices[model][ckpt][iconfmat], recall_matrices[model][ckpt][iconfmat], precision_mean, recall_mean = \
                      normalize_confusion_matrix(confusion_matrix)
                plot_confusion_matrix(fig, ax,
                                      confusion_matrix,
                                      precision_matrices[model][ckpt][iconfmat],
                                      recall_matrices[model][ckpt][iconfmat],
                                      len(labels)<10,
                                      "",
                                      labels if FLAGS.loss=='exclusive' else
                                          [labels[iconfmat], FLAGS.overlapped_prefix+labels[iconfmat]],
                                      precision_mean, recall_mean)
                P,R = precision_mean, recall_mean
                if FLAGS.loss=='exclusive' and 2*P*R/(P+R) > best_F1[model][1]:
                    best_F1[model] = (ckpt, 2*P*R/(P+R))
            if FLAGS.loss=='overlapped':
                ax = plt.subplot(nrows,ncols,1)
                summed_confusion_matrix = np.sum(confusion_matrices[model][ckpt], axis=0)
                precision_matrix, recall_matrix, precision_mean, recall_mean = \
                      normalize_confusion_matrix(summed_confusion_matrix)
                plot_confusion_matrix(fig, ax,
                                      summed_confusion_matrix,
                                      precision_matrix,
                                      recall_matrix,
                                      len(labels)<10,
                                      "",
                                      ["song", FLAGS.overlapped_prefix+"song"],
                                      precision_mean, recall_mean)
                P,R = precision_mean, recall_mean
                if 2*P*R/(P+R) > best_F1[model][1]:
                    best_F1[model] = (ckpt, 2*P*R/(P+R))
            fig.tight_layout()
            plt.savefig(os.path.join(FLAGS.logdir,model,'confusion-matrix.ckpt-'+ckpt+'.pdf'))
            plt.close()

    print('ckpts =', [best_F1[k][0] for k in keys_to_plot])
    print('labels =', labels)
    print('models =', keys_to_plot)
    if FLAGS.loss=='exclusive':
        precisions = {model: [100*precision_matrices[model][ckpt][0][ilabel][ilabel]
                       for ilabel in range(len(labels))]
                      for model in keys_to_plot}
        recalls = {model: [100*recall_matrices[model][ckpt][0][ilabel][ilabel]
                    for ilabel in range(len(labels))]
                   for model in keys_to_plot}
    else:
        precisions = {model: [100*precision_matrices[model][ckpt][ilabel][0][0]
                       for ilabel in range(len(labels))]
                      for model in keys_to_plot}
        recalls = {model: [100*recall_matrices[model][ckpt][ilabel][0][0]
                    for ilabel in range(len(labels))]
                   for model in keys_to_plot}
    print('precisions =', [precisions[model] for model in keys_to_plot])
    print('recalls =', [recalls[model] for model in keys_to_plot])



    if FLAGS.loss=='exclusive':
        fig = plt.figure(figsize=(6.4, 4.8))
        ax = plt.subplot(1,1,1)
    else:
        nrows, ncols = layout(1+len(labels))
        fig = plt.figure(figsize=(ncols*4.8, nrows*3.6))
        ax = plt.subplot(nrows,ncols,1)
  
    summed_confusion_matrix = None
    for model in best_F1.keys():
      ckpt = best_F1[model][0]
      if summed_confusion_matrix is None:
          summed_confusion_matrix = np.array(confusion_matrices[model][ckpt])
      else:
          summed_confusion_matrix += confusion_matrices[model][ckpt]
    summed_confusion_matrix = np.sum(summed_confusion_matrix, axis=0)
    precision_summed_matrix, recall_summed_matrix, precision_summed, recall_summed = \
          normalize_confusion_matrix(summed_confusion_matrix)
    plot_confusion_matrix(fig, ax,
                          summed_confusion_matrix, \
                          precision_summed_matrix, recall_summed_matrix, \
                          len(labels)<10,
                          "",
                          labels if FLAGS.loss=='exclusive' else
                              ["song", FLAGS.overlapped_prefix+"song"],
                          precision_summed, recall_summed)
    if FLAGS.loss=='overlapped':
        for ilabel in range(len(labels)):
            ax = plt.subplot(nrows,ncols,2+ilabel)
            summed_confusion_matrix = None
            for model in best_F1.keys():
              ckpt = best_F1[model][0]
              if summed_confusion_matrix is None:
                  summed_confusion_matrix = np.array(confusion_matrices[model][ckpt][ilabel])
              else:
                  summed_confusion_matrix += confusion_matrices[model][ckpt][ilabel]
            precision_summed_matrix, recall_summed_matrix, precision_summed, recall_summed = \
                  normalize_confusion_matrix(summed_confusion_matrix)
            plot_confusion_matrix(fig, ax,
                                  summed_confusion_matrix, \
                                  precision_summed_matrix, recall_summed_matrix, \
                                  len(labels)<10,
                                  "",
                                  [labels[ilabel], FLAGS.overlapped_prefix+labels[ilabel]],
                                  precision_summed, recall_summed)

    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'confusion-matrix.pdf'))
    plt.close()


    fig = plt.figure(figsize=(2*6.4, 4.8))
  
    ax = plt.subplot(1,2,1)
    colors = plt.rcParams["axes.prop_cycle"].by_key()["color"]
    if len(colors)<len(labels):
      colors = list(islice(cycle(colors), len(labels)))
      print("WARNING: not enough colors in default palette; some will be used again")
    precisions_mean, recalls_mean = [], []
    for (ilabel,label) in enumerate(labels):
      precisions_all, recalls_all = [], []
      for model in keys_to_plot:
        ckpt = best_F1[model][0]
        precisions_all.append(precisions[model][ilabel])
        recalls_all.append(recalls[model][ilabel])
        line, = ax.plot(recalls_all[-1], precisions_all[-1], 'o',
                        markeredgecolor='w', color=colors[ilabel])
        if model==keys_to_plot[0]:
          line.set_label(label)
      precisions_mean.append(np.nanmean(precisions_all))
      recalls_mean.append(np.nanmean(recalls_all))
      ax.plot(recalls_mean[-1], precisions_mean[-1], 'o',
              markeredgecolor='k', color=colors[ilabel])
    label_precisions_recall(ax, recalls_mean, precisions_mean, "")

    ax = fig.add_subplot(1,2,2)
    colors = plt.rcParams["axes.prop_cycle"].by_key()["color"]
    if len(colors)<len(keys_to_plot):
      colors = list(islice(cycle(colors), len(keys_to_plot)))
      print("WARNING: not enough colors in default palette; some will be used again")
    precisions_mean, recalls_mean = [], []
    for (imodel,model) in enumerate(keys_to_plot):
      precisions_all, recalls_all = [], []
      for (ilabel,label) in enumerate(labels):
        ckpt = best_F1[model][0]
        precisions_all.append(precisions[model][ilabel])
        recalls_all.append(recalls[model][ilabel])
        line, = ax.plot(recalls_all[-1], precisions_all[-1], 'o',
                        markeredgecolor='w', color=colors[imodel])
        if label==labels[0]:
          line.set_label(model+'/ckpt-'+best_F1[model][0])
      precisions_mean.append(np.nanmean(precisions_all))
      recalls_mean.append(np.nanmean(recalls_all))
      ax.plot(recalls_mean[-1], precisions_mean[-1], 'o',
              markeredgecolor='k', color=colors[imodel])
    label_precisions_recall(ax, recalls_mean, precisions_mean, "")

    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'precision-recall.pdf'))
    plt.close()
  

    if FLAGS.parallelize!=0:
      from multiprocessing import Pool
      nprocs = os.cpu_count() if FLAGS.parallelize==-1 else FLAGS.parallelize
      pool = Pool(nprocs)
      results = []
  
    for model in keys_to_plot:
      for ckpt in [int(x.split('-')[1][:-4]) for x in \
                   filter(lambda x: 'validation' in x and x.endswith('.npz'), \
                          os.listdir(os.path.join(FLAGS.logdir,model)))]:
  
        if FLAGS.parallelize!=0:
          results.append(pool.apply_async(doit,
              (FLAGS.logdir, model, ckpt, labels, FLAGS.nprobabilities, error_ratios,
               FLAGS.loss, FLAGS.overlapped_prefix)))
        else:
          doit(FLAGS.logdir, model, ckpt, labels, FLAGS.nprobabilities, error_ratios,
               FLAGS.loss, FLAGS.overlapped_prefix)
  
    if FLAGS.parallelize!=0:
      for result in results:
        result.get()
      pool.close()
  
if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--logdir',
      type=str,
      default='/tmp/speech_commands_train',
      help='Directory to write event logs and checkpoint.')
  parser.add_argument(
      '--error_ratios',
      type=str,
      default='2,1,0.5',
      help='')
  parser.add_argument(
      '--nprobabilities',
      type=int,
      default=50,
      help='')
  parser.add_argument(
      '--loss',
      type=str,
      default='exclusive',
      choices=['exclusive', 'overlapped'],
      help='Sigmoid cross entropy is used for "overlapped" labels while softmax cross entropy is used for "exclusive" labels.')
  parser.add_argument(
      '--overlapped_prefix',
      type=str,
      default='not_',
      help='When `loss` is `overlapped`, a label starting which this string indicates the absence of the class.  E.g. `song` and `not_song`.')
  parser.add_argument(
      '--parallelize',
      type=int,
      default=0,
      help='')

  FLAGS, unparsed = parser.parse_known_args()

  print(str(datetime.now())+": start time")
  with open(os.path.join(repodir, "VERSION.txt"), 'r') as fid:
    print('SongExplorer version = '+fid.read().strip().replace('\n',', '))
  print("hostname = "+socket.gethostname())
  
  try:
    main()
  
  except Exception as e:
    print(e)
  
  finally:
    if hasattr(os, 'sync'):
      os.sync()
    print(str(datetime.now())+": finish time")
