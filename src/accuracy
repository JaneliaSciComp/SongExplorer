#!/usr/bin/env python3

# generate learning curves, confusion matrices, precision-recall curves, thresholds, etc.
 
# e.g.
# $SONGEXPLORER_BIN accuracy \
#     --logdir=trained-classifier \
#     --error_ratios=2,1,0.5 \
#     --nprobabilities=50 \
#     --parallelize=-1

import argparse
import sys
import os
import re
import numpy as np
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
#plt.ion()
import csv
from natsort import natsorted, index_natsorted
import matplotlib.cm as cm
from matplotlib.patches import Rectangle
from datetime import datetime
import socket
import statistics

repodir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))

sys.path.append(os.path.join(repodir, "src"))
from lib import *

from scipy import interpolate
from scipy import stats
import math
  
FLAGS = None

def plot_metrics_parameterized_by_threshold(thresholds, \
                                            metric1_data, metric2_data, \
                                            metric1_label, metric2_label, \
                                            areas, labels, probabilities, ratios):
  speciess = sorted(list(set([x.split('-')[0] for x in labels])))
  if len(speciess)<len(labels):
    ncols = len(speciess)
    nrows = max([sum([x.split('-')[0]==s for x in labels]) for s in speciess])
    colcounts = np.zeros(ncols, dtype=np.uint8)
  else:
    nrows, ncols = layout(len(labels))
  fig=plt.figure(figsize=(3.2*ncols, 2.4*nrows))
  for ilabel in range(len(labels)):
    if len(speciess)<len(labels):
      icol = np.argmax([labels[ilabel].split('-')[0]==s for s in speciess])
      irow = colcounts[icol]
      colcounts[icol] += 1
      ax=plt.subplot(nrows,ncols,irow*ncols+icol+1)
    else:
      ax=plt.subplot(nrows,ncols,ilabel+1)
    if ilabel==0:
      ax.set_xlabel(metric2_label)
      ax.set_ylabel(metric1_label)
    ax.set_title(labels[ilabel]+',  area='+str(np.round(areas[labels[ilabel]],3)))
    ax.set_xlim(0,1)
    ax.set_ylim(0,1)
    line, = plt.plot(metric2_data[labels[ilabel]], metric1_data[labels[ilabel]])
    for (ithreshold,threshold) in enumerate(thresholds[labels[ilabel]]):
      if np.isnan(threshold):
        continue
      iprobability = np.argmin(np.abs(probabilities[labels[ilabel]]-threshold))
      if ratios[ithreshold]==1:
        plt.plot(metric2_data[labels[ilabel]][iprobability], \
                 metric1_data[labels[ilabel]][iprobability], 'ro')
      else:
        plt.plot(metric2_data[labels[ilabel]][iprobability], \
                 metric1_data[labels[ilabel]][iprobability], 'rx')
      whichside = 'right' if metric2_data[labels[ilabel]][iprobability]>0.5 else 'left'
      ax.annotate(str(np.round(threshold,3))+' ',
                  xy=(metric2_data[labels[ilabel]][iprobability], \
                      metric1_data[labels[ilabel]][iprobability]),
                  color="r", verticalalignment='top', horizontalalignment=whichside)
  fig.tight_layout()

def plot_probability_density(test_ground_truth, test_logits, ratios, thresholds, labels):
  speciess = sorted(list(set([x.split('-')[0] for x in labels])))
  if len(speciess)<len(labels):
    ncols = len(speciess)
    nrows = max([sum([x.split('-')[0]==s for x in labels]) for s in speciess])
    colcounts = np.zeros(ncols, dtype=np.uint8)
  else:
    nrows, ncols = layout(len(labels))
  x = np.arange(0, 1.01, 0.01)
  fig=plt.figure(figsize=(3.2*ncols, 2.4*nrows))
  for ilabel in range(len(labels)):
    if len(speciess)<len(labels):
      icol = np.argmax([labels[ilabel].split('-')[0]==s for s in speciess])
      irow = colcounts[icol]
      colcounts[icol] += 1
      ax=plt.subplot(nrows,ncols,irow*ncols+icol+1)
    else:
      ax=plt.subplot(nrows,ncols,ilabel+1)
    if ilabel==0:
      ax.set_xlabel('Probability')
      ax.set_ylabel('Density')
    ax.set_title(labels[ilabel])
    for (ithreshold,threshold) in enumerate(thresholds[labels[ilabel]]):
      if np.isnan(threshold):
        continue
      if ratios[ithreshold]==1:
        plt.axvline(x=threshold, color='k', linestyle='dashed')
      else:
        plt.axvline(x=threshold, color='k', linestyle='dotted')
    for gt in range(len(labels)):
      igt = test_ground_truth==gt
      if sum(igt)==0:
        continue
      xdata = test_logits[igt,ilabel]
      xdata = np.minimum(np.finfo(float).max, np.exp(xdata))
      xdata = xdata / (xdata + 1)
      if len(xdata)<2 or any(np.isnan(xdata)):
        continue
      density = stats.gaussian_kde(xdata)
      y = density(x)
      line, = plt.plot(x, y)
      if ilabel==gt:
        ax.set_ylim(0,max(y))
      if ilabel==0:
        line.set_label(labels[gt])
  fig.tight_layout()
  return fig.legend(loc='lower left')

def doit(logdir, key_to_plot, ckpt, labels, nprobabilities, error_ratios):
  validation_sounds, validation_ground_truth, validation_logits = \
        read_logits(logdir, key_to_plot, ckpt)

  probabilities, thresholds, precisions, recalls, sensitivities, specificities, \
        pr_areas, roc_areas = calculate_precision_recall_specificity( \
        validation_ground_truth, validation_logits, labels, nprobabilities, \
        error_ratios)

  save_thresholds(logdir, key_to_plot, ckpt, thresholds, error_ratios, labels)

  plot_metrics_parameterized_by_threshold(thresholds, precisions, recalls, \
        'precision = Tp/(Tp+Fp)', 'recall = Tp/(Tp+Fn)', pr_areas, \
        labels, probabilities, error_ratios)
  plt.savefig(os.path.join(logdir,key_to_plot,'precision-recall.ckpt-'+str(ckpt)+'.pdf'))
  plt.close()

  plot_metrics_parameterized_by_threshold(thresholds, specificities, \
        sensitivities, 'specificity = Tn/(Tn+Fp)', 'sensitivity = Tp/(Tp+Fn)', \
        roc_areas, labels, probabilities, error_ratios)
  plt.savefig(os.path.join(logdir, key_to_plot, \
                           'specificity-sensitivity.ckpt-'+str(ckpt)+'.pdf'))
  plt.close()

  already_written = set()
  predictions_path = os.path.join(logdir,key_to_plot,'predictions.ckpt-'+str(ckpt))
  if not os.path.isdir(predictions_path):
    os.mkdir(os.path.join(logdir,key_to_plot,'predictions.ckpt-'+str(ckpt)))
  wavfiles = set([x['file'] for x in validation_sounds])
  for subdir in list(set([os.path.basename(os.path.split(x)[0]) for x in wavfiles])):
    with open(os.path.join(logdir, key_to_plot, 'predictions.ckpt-'+str(ckpt), \
                           subdir+'-mistakes.csv'), \
              'w', newline='') as csvfile:
      csvwriter = csv.writer(csvfile, lineterminator='\n')
      for i in range(len(validation_ground_truth)):
        if os.path.basename(os.path.split(validation_sounds[i]['file'])[0]) != subdir:
          continue
        classified_as = np.argmax(validation_logits[i,:])
        id = validation_sounds[i]['file'] + str(validation_sounds[i]['ticks']) + \
             validation_sounds[i]['label'] + labels[classified_as]
        if id in already_written:
          continue
        already_written |= set([id])
        csvwriter.writerow([os.path.basename(validation_sounds[i]['file']),
              validation_sounds[i]['ticks'][0], validation_sounds[i]['ticks'][1],
              'correct' if classified_as == validation_ground_truth[i] else 'mistaken',
              labels[classified_as],
              validation_sounds[i]['label']])

  lgd = plot_probability_density(validation_ground_truth, validation_logits, \
                                 error_ratios, thresholds, labels)
  plt.savefig(os.path.join(logdir, key_to_plot, \
                           'probability-density.ckpt-'+str(ckpt)+'.pdf'), \
              bbox_extra_artists=(lgd,), bbox_inches='tight')

  plt.close()
  return 1

def main():
    flags = vars(FLAGS)
    for key in sorted(flags.keys()):
      print('%s = %s' % (key, flags[key]))

    error_ratios = [float(x) for x in FLAGS.error_ratios.split(',')]
  
    train_accuracy, train_loss, train_time, train_step, \
          validation_precision, validation_recall, \
          validation_precision_mean, validation_recall_mean, \
          validation_time, validation_step, \
          _, _, _, _, \
          labels_touse, label_counts, _, _, batch_size, _ = \
          read_logs(FLAGS.logdir)
    training_set_size = {k: len(label_counts[k]["training"]) * \
                            np.max(list(label_counts[k]["training"].values())) \
                         for k in label_counts.keys()}
  
    keys_to_plot = natsorted(train_step.keys())
  
    nrows, ncols = layout(len(keys_to_plot))
    fig = plt.figure(figsize=(6.4*ncols, 4.8*nrows))
    fig.subplots_adjust(bottom=0.2)
    for (imodel,model) in enumerate(keys_to_plot):
      ax4 = fig.add_subplot(nrows, ncols, imodel+1)
      nx = len(train_step[model])
      idx = range(nx) if nx<10000 else np.round(np.linspace(0, nx-1, 10000)).astype(int)
      ax4.plot(np.array(train_step[model])[idx],
               np.array(train_loss[model])[idx],
               'g', alpha=0.5, label='Loss')
      ax4.set_xlabel('Step')
      ax4.set_ylabel('Loss', color='g')
      ax4.tick_params(axis='y', labelcolor='g')
      ax1 = ax4.twinx()
      ax1.plot(np.array(train_step[model])[idx],
               np.array(train_accuracy[model])[idx],
               'b', alpha=0.5, label='Train')
      validation_intervals = list(zip(validation_step[model][0:-1], \
                                      validation_step[model][1:]))
      train_accuracy_ave = [np.mean(train_accuracy[model][train_step[model].index(x): \
                                                          train_step[model].index(y)]) \
                            for (x,y) in validation_intervals]
      ax1.plot([(x+y)/2 for (x,y) in validation_intervals], train_accuracy_ave, 'c', label='Train mean')
      train_loss_ave = [np.mean(train_loss[model][train_step[model].index(x): \
                                                  train_step[model].index(y)]) \
                            for (x,y) in validation_intervals]
      ax4.plot([(x+y)/2 for (x,y) in validation_intervals], train_loss_ave, 'm', label='Loss mean')
      if validation_recall_mean[model]:
        ax1.plot(validation_step[model], validation_recall_mean[model], 'r', label='Validation')
        ax1.set_title(model+"   "+str(round(validation_recall_mean[model][-1],1))+'%')
        ax1.set_ylim(bottom=min(validation_recall_mean[model]))
      ax1.set_ylim(top=100)
      ax1.set_ylabel('Overall recall')
      ax1.set_xlim(0,1+len(train_step[model]))
      if imodel==len(keys_to_plot)-1:
        handles1, labels1 = ax1.get_legend_handles_labels()
        handles4, labels4 = ax4.get_legend_handles_labels()
        ax1.legend(handles1+handles4, labels1+labels4, loc=(1.25, 0.0)) #loc='upper center')
      ax2 = ax4.twiny()
      ax2.xaxis.set_ticks_position("bottom")
      ax2.xaxis.set_label_position("bottom")
      ax2.spines["bottom"].set_position(("axes", -0.2))
      ax2.set_frame_on(True)
      ax2.patch.set_visible(False)
      for k in ax2.spines.keys():
          ax2.spines[k].set_visible(False)
      ax2.spines["bottom"].set_visible(True)
      scaled_data,units = choose_units(train_time[model])
      ax2.set_xlim(scaled_data[0],scaled_data[-1])
      ax2.set_xlabel('Time ('+units+')')
      ax3 = ax4.twiny()
      ax3.xaxis.set_ticks_position("bottom")
      ax3.xaxis.set_label_position("bottom")
      ax3.spines["bottom"].set_position(("axes", -0.4))
      ax3.set_frame_on(True)
      ax3.patch.set_visible(False)
      for k in ax3.spines.keys():
          ax3.spines[k].set_visible(False)
      ax3.spines["bottom"].set_visible(True)
      step2epoch = batch_size[model] / training_set_size[model]
      ax3.set_xlim(train_step[model][0]*step2epoch, \
                   train_step[model][-1]*step2epoch)
      ax3.set_xlabel('Epoch')
  
    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'train-validation-loss.pdf'))
    plt.close()
  
    nrows, ncols = layout(len(keys_to_plot))
    for (ilabel,label) in enumerate(labels_touse[keys_to_plot[0]]):
      fig = plt.figure(figsize=(6.4*ncols, 4.8*nrows))
      ax = []
      minp=100
      minr=100
      for (imodel,model) in enumerate(keys_to_plot):
        ax.append(fig.add_subplot(nrows, ncols, imodel+1))
        precision = [100*x[ilabel] for x in validation_precision[model]]
        recall = [100*x[ilabel] for x in validation_recall[model]]
        minp = min(minp, min(precision))
        minr = min(minr, min(recall))
        ax[imodel].set_prop_cycle(color=[cm.viridis(1.*i/max(1,len(recall)-2)) \
                                         for i in range(len(recall)-1)])
        for i in range(len(recall)-1):
          ax[imodel].plot(recall[i:i+2], precision[i:i+2])
        ax[imodel].annotate(str(validation_step[model][0]),
                    xy=(recall[0], precision[0]),
                    color="r", verticalalignment='center', horizontalalignment='center')
        ax[imodel].annotate(str(validation_step[model][-1]),
                    xy=(recall[-1], precision[-1]),
                    color="r", verticalalignment='center', horizontalalignment='center')
        ibestF1 = np.argmax([2*p*r/(p+r) if (p+r)>0 else np.nan \
                             for (p,r) in zip(precision,recall)])
        ax[imodel].annotate(str(validation_step[model][ibestF1]),
                    xy=(recall[ibestF1], precision[ibestF1]),
                    color="r", verticalalignment='center', horizontalalignment='center')
        ax[imodel].set_xlabel('Recall')
        ax[imodel].set_ylabel('Precision')
        ax[imodel].set_title(model)
      for imodel in range(len(keys_to_plot)):
        ax[imodel].set_ylim([minp,100])
        ax[imodel].set_xlim([minr,100])
      fig.tight_layout()
      plt.savefig(os.path.join(FLAGS.logdir,'validation-PvR-'+label+'.pdf'))
      plt.close()
  
  
    nrows, ncols = layout(len(labels_touse[keys_to_plot[0]]))
    fig = plt.figure(figsize=(6.4*ncols, 4.8*nrows))
    lines=[]
    isort = index_natsorted(labels_touse[keys_to_plot[0]])
    for ilabel in range(len(isort)):
      ax = fig.add_subplot(nrows, ncols, ilabel+1)
      for (imodel,model) in enumerate(keys_to_plot):
        F1 = [2*p[isort[ilabel]]*r[isort[ilabel]]/(p[isort[ilabel]]+r[isort[ilabel]]) \
              if (p[isort[ilabel]]+r[isort[ilabel]])>0 else np.nan \
              for (p,r) in zip(validation_precision[model], validation_recall[model])]
        line, = ax.plot(validation_step[model], F1)
        if ilabel==0:
          lines.append(line)
        ax.set_ylim(top=1)
        ax.set_xlabel('Step')
        ax.set_ylabel('F1 = 2PR/(P+R)')
        ax.set_title(labels_touse[keys_to_plot[0]][isort[ilabel]])
      ax3 = ax.twiny()
      ax3.xaxis.set_ticks_position("bottom")
      ax3.xaxis.set_label_position("bottom")
      ax3.spines["bottom"].set_position(("axes", -0.15))
      ax3.set_frame_on(True)
      ax3.patch.set_visible(False)
      for k in ax3.spines.keys():
          ax3.spines[k].set_visible(False)
      ax3.spines["bottom"].set_visible(True)
      step2epoch = batch_size[model] / training_set_size[model]
      ax3.set_xlim(validation_step[model][0]*step2epoch, \
                   validation_step[model][-1]*step2epoch)
      ax3.set_xlabel('Epoch')
  
    lgd = fig.legend(lines, keys_to_plot, ncol=len(keys_to_plot),
                     bbox_to_anchor=(0.0,-0.02), loc="lower left")
    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'validation-F1.pdf'),
                bbox_extra_artists=(lgd,), bbox_inches='tight')
    plt.close()
  
    if not len(validation_recall_mean[keys_to_plot[0]]):
        sys.exit()
  
    if len(keys_to_plot)>1:
      fig = plt.figure(figsize=(6.4*1.5, 4.8))
  
      ax = fig.add_subplot(2,3,1)
      for model in keys_to_plot:
        scaled_validation_time, units = choose_units(validation_time[model])
  
        line, = ax.plot(validation_step[model], validation_recall_mean[model])
        line.set_label(model)
      ax.set_ylim(top=100)
      ax.set_xlabel('Step')
      ax.set_ylabel('Overall recall')
      #ax.legend(loc='lower right')
  
      ax = fig.add_subplot(2,3,2)
      for model in keys_to_plot:
        ax.plot([x*batch_size[model]/training_set_size[model] \
                 for x in validation_step[model]], validation_recall_mean[model])
      ax.set_ylim(top=100)
      ax.set_xlabel('Epoch')
      ax.set_ylabel('Overall recall')
  
      ax = fig.add_subplot(2,3,3)
      for model in keys_to_plot:
        idx = min(len(scaled_validation_time), len(validation_recall_mean[model]))
        line, = ax.plot(scaled_validation_time[:idx], validation_recall_mean[model][:idx])
        line.set_label(model)
      ax.set_ylim(top=100)
      ax.set_xlabel('Time ('+units+')')
      ax.set_ylabel('Overall recall')
  
      ax = fig.add_subplot(2,3,4)
      for model in keys_to_plot:
        epoch = [x*batch_size[model]/training_set_size[model]
                 for x in validation_step[model]]
        idx = min(len(scaled_validation_time), len(epoch))
        ax.plot(scaled_validation_time[:idx], epoch[:idx])
      ax.set_xlabel('Time ('+units+')')
      ax.set_ylabel('Epoch')
  
      ax = fig.add_subplot(2,3,5)
      for model in keys_to_plot:
        idx = min(len(scaled_validation_time), len(validation_step[model]))
        ax.plot(scaled_validation_time[:idx], validation_step[model][:idx])
      ax.set_xlabel('Time ('+units+')')
      ax.set_ylabel('Step')
  
      ax = fig.add_subplot(2,3,6)
      for model in keys_to_plot:
        line, = ax.plot([x*batch_size[model]/training_set_size[model] \
                         for x in validation_step[model]], validation_step[model])
        line.set_label(model)
      ax.set_xlabel('Epoch')
      ax.set_ylabel('Step')
      ax.legend(loc=(1.05, 0.0))
  
      fig.tight_layout()
  
      plt.savefig(os.path.join(FLAGS.logdir,'validation-overlay.pdf'))
      plt.close()

      fig = plt.figure(figsize=(6.4, 4.8))
  
      model_validation_recall = np.zeros(len(validation_recall_mean[keys_to_plot[0]]))
      nmodels = 0
      for model in keys_to_plot:
        if validation_step[model] != validation_step[keys_to_plot[0]]:
          print("WARNING: not all checkpoint steps are the same for "+model)
          continue
        model_validation_recall += validation_recall_mean[model]
        model_validation_step = validation_step[model]
        nmodels += 1
      ax = fig.add_subplot(1,1,1)
      line, = ax.plot(model_validation_step, model_validation_recall / nmodels)
      line.set_label("model average")
      ax.set_ylim(top=100)
      ax.set_xlabel('Step')
      ax.set_ylabel('Overall recall')
      ax.legend(loc=(1.05, 0.0))
      ax.grid(True)
  
      fig.tight_layout()
  
      plt.savefig(os.path.join(FLAGS.logdir,'validation-average.pdf'))
      plt.close()
  
    summed_confusion_matrix, confusion_matrices, labels = \
        parse_confusion_matrices(FLAGS.logdir, next(iter(keys_to_plot)).split('_')[0])
  
    recall_matrices={}
    precision_matrices={}
    precisions_mean={}
    recalls_mean={}
    for model in keys_to_plot:
      precision_matrices[model], recall_matrices[model], precisions_mean[model], recalls_mean[model] = \
            normalize_confusion_matrix(confusion_matrices[model])
  
    precision_summed_matrix, recall_summed_matrix, precision_summed, recall_summed = \
          normalize_confusion_matrix(summed_confusion_matrix)
  
    from mpl_toolkits.axes_grid1 import make_axes_locatable
  
    fig = plt.figure(figsize=(3*6.4, 4.8))
  
    ax = plt.subplot(1,3,1)
    plot_confusion_matrix(ax, summed_confusion_matrix, \
                          precision_summed_matrix, recall_summed_matrix, \
                          numbers=len(labels)<10)
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.1)
    fig.colorbar(cm.ScalarMappable(norm=mpl.colors.Normalize(vmin=0, vmax=100),
                                   cmap=cm.viridis),
                 cax=cax, ticks=[0,100], use_gridspec=True)
    ax.set_xticklabels(labels, rotation=40, ha='right')
    ax.set_yticklabels(labels)
    ax.invert_yaxis()
    ax.set_xlabel('Classification')
    ax.set_ylabel('Annotation')
    ax.set_title("P="+str(round(precision_summed,1))+"%   "+
                 "R="+str(round(recall_summed,1))+"%")
  
    ax = plt.subplot(1,3,2)
    precisions_all = []
    recalls_all = []
    for model in keys_to_plot:
      ax.set_prop_cycle(None)
      for (ilabel,label) in enumerate(labels):
        precisions_all.append(100*precision_matrices[model][ilabel][ilabel])
        recalls_all.append(100*recall_matrices[model][ilabel][ilabel])
        line, = ax.plot(recalls_all[-1], precisions_all[-1], 'o', markeredgecolor='k')
        if model==keys_to_plot[0]:
          line.set_label(label)

    if len(recalls_all)>1:
      ax.autoscale_view()
      ax.set_autoscale_on(False)
      miny = ax.get_ylim()[0]
      minx = ax.get_xlim()[0]
      x = statistics.mean(recalls_all)
      w = statistics.stdev(recalls_all)
      avebox = Rectangle((x-w,miny),2*w,100)
      ax.plot([x,x],[miny,100],'w-')
      pc = PatchCollection([avebox], facecolor='lightgray', alpha=0.5)
      ax.add_collection(pc)
      y = statistics.mean(precisions_all)
      h = statistics.stdev(precisions_all)
      avebox = Rectangle((minx,y-h),100,2*h)
      ax.plot([minx,100],[y,y],'w-')
      pc = PatchCollection([avebox], facecolor='lightgray', alpha=0.5)
      ax.add_collection(pc)
    else:
      x,y,w,h = recalls_all[0], precisions_all[0], 0, 0

    ax.set_xlim(right=100)
    ax.set_ylim(top=100)
    ax.set_xlabel('Recall (%)')
    ax.set_ylabel('Precision (%)')
    ax.set_title("P="+str(round(y,1))+"+/-"+str(round(h,1))+"%   "+
                 "R="+str(round(x,1))+"+/-"+str(round(w,1))+"%")
    ax.legend(loc=(1.05, 0.0))
  
    ax = fig.add_subplot(1,3,3)
    precisions_mean_ordered = [precisions_mean[k] for k in keys_to_plot]
    recalls_mean_ordered = [recalls_mean[k] for k in keys_to_plot]
    print('models=', keys_to_plot)
    print('precisions=', precisions_mean_ordered)
    print('recalls=', recalls_mean_ordered)

    for model in keys_to_plot:
      ax.set_prop_cycle(None)
      line, = ax.plot(recalls_mean[model],
                      precisions_mean[model],
                      'o', markeredgecolor='k')
      line.set_label(model)
  
    if len(recalls_mean_ordered)>1:
      ax.autoscale_view()
      ax.set_autoscale_on(False)
      miny = ax.get_ylim()[0]
      minx = ax.get_xlim()[0]
      x = statistics.mean(recalls_mean_ordered)
      w = statistics.stdev(recalls_mean_ordered)
      avebox = Rectangle((x-w,miny),2*w,100)
      ax.plot([x,x],[miny,100],'w-')
      pc = PatchCollection([avebox], facecolor='lightgray', alpha=0.5)
      ax.add_collection(pc)
      y = statistics.mean(precisions_mean_ordered)
      h = statistics.stdev(precisions_mean_ordered)
      avebox = Rectangle((minx,y-h),100,2*h)
      ax.plot([minx,100],[y,y],'w-')
      pc = PatchCollection([avebox], facecolor='lightgray', alpha=0.5)
      ax.add_collection(pc)
    else:
      x,y,w,h - recalls_mean_ordered[0], precisions_mean_ordered[0], 0, 0

    ax.set_xlim(right=100)
    ax.set_ylim(top=100)
    ax.set_xlabel('Recall (%)')
    ax.set_ylabel('Precision (%)')
    ax.set_title("P="+str(round(y,1))+"+/-"+str(round(h,1))+"%   "+
                 "R="+str(round(x,1))+"+/-"+str(round(w,1))+"%")
    ax.legend(loc=(1.05, 0.0))
  
    fig.tight_layout()
    plt.savefig(os.path.join(FLAGS.logdir,'accuracy.pdf'))
    plt.close()
  
    if len(keys_to_plot)>1:
      plot_confusion_matrices(confusion_matrices,
                              precision_matrices,
                              recall_matrices,
                              labels, precisions_mean, recalls_mean, keys_to_plot,
                              numbers=len(labels)<10)
      plt.savefig(os.path.join(FLAGS.logdir,'confusion-matrices.pdf'))
      plt.close()
  
    if FLAGS.parallelize!=0:
      from multiprocessing import Pool
      nprocs = os.cpu_count() if FLAGS.parallelize==-1 else FLAGS.parallelize
      pool = Pool(nprocs)
      results = []
  
    for model in recalls_mean:
      for ckpt in [int(x.split('-')[1][:-4]) for x in \
                   filter(lambda x: 'validation' in x and x.endswith('.npz'), \
                          os.listdir(os.path.join(FLAGS.logdir,model)))]:
  
        if FLAGS.parallelize!=0:
          results.append(pool.apply_async(doit, (FLAGS.logdir,model,ckpt,labels,FLAGS.nprobabilities,error_ratios)))
        else:
          doit(FLAGS.logdir, model, ckpt, labels, FLAGS.nprobabilities, error_ratios)
  
    if FLAGS.parallelize!=0:
      for result in results:
        result.get()
      pool.close()
  
if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--logdir',
      type=str,
      default='/tmp/speech_commands_train',
      help='Directory to write event logs and checkpoint.')
  parser.add_argument(
      '--error_ratios',
      type=str,
      default='2,1,0.5',
      help='')
  parser.add_argument(
      '--nprobabilities',
      type=int,
      default=50,
      help='')
  parser.add_argument(
      '--parallelize',
      type=int,
      default=0,
      help='')

  FLAGS, unparsed = parser.parse_known_args()

  print(str(datetime.now())+": start time")
  with open(os.path.join(repodir, "VERSION.txt"), 'r') as fid:
    print('SongExplorer version = '+fid.read().strip().replace('\n',', '))
  print("hostname = "+socket.gethostname())
  
  try:
    main()
  
  except Exception as e:
    print(e)
  
  finally:
    if hasattr(os, 'sync'):
      os.sync()
    print(str(datetime.now())+": finish time")
