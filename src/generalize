#!/usr/bin/env python3

# train several networks, withholding different subsets of the recordings to test upon

# e.g. generalize \
#     --context=204.8 \
#     --shiftby=0.0 \
#     --optimizer=Adam \
#     --loss=exclusive \
#     --overlapped_prefix=not_ \
#     --learning_rate=0.0002 \
#     --audio_read_plugin=load_wav \
#     --audio_read_plugin_kwargs="{}" \
#     --video_read_plugin=load-avi-mp4-mov \
#     --video_read_plugin_kwargs="{}" \
#     --video_findfile=same-basename \
#     --video_bkg_frames=1000 \
#     --data_loader_queuesize=0 \
#     --data_loader_maxprocs=1 \
#     --model_architecture=convolutional \
#     --model_parameters='{"representation":"waveform", "window":6.4, "stride":1.6, "mel_dct":"7,7", "dropout":0.5, "kernel_sizes":5,128", last_conv_width":130, "nfeatures":"256,256", "dilate_after_layer":65535, "stride_after_layer":65535, "connection_type":"plain"}' \
#     --logdir=`pwd`/leave-one-out \
#     --data_dir=`pwd`/groundtruth-data \
#     --labels_touse=mel-pulse,mel-sine,ambient,other \
#     --kinds_touse=annotated \
#     --nsteps=50 \
#     --restore_from='' \
#     --save_and_validate_period=10 \
#     --mini_batch=32 \
#     --testing_files="" \
#     --time_units=ms \
#     --freq_units=Hz \
#     --time_scale=0.001 \
#     --freq_scale=1 \
#     --audio_tic_rate=5000 \
#     --audio_nchannels=1 \
#     --video_frame_rate=0 \
#     --video_frame_width=0 \
#     --video_frame_height=0 \
#     --video_channels=0 \
#     --batch_seed=_1 \
#     --weights_seed=_1 \
#     --augment_volume=1,1 \
#     --augment_noise=0,0 \
#     --augment_dc=0,0 \
#     --augment_reverse=no \
#     --augment_invert=no \
#     --deterministic=0 \
#     --igpu=0 \
#     --ioffset=3 \
#     --subsets=20161207T102314_ch1_p1.wav,20161207T102314_ch1_p2.wav,20161207T102314_ch1_p3.wav PS_20130625111709_ch3_p1.wav,PS_20130625111709_ch3_p2.wav,PS_20130625111709_ch3_p3.wav

import argparse
import os
import sys
from subprocess import run, PIPE, STDOUT
import asyncio
import signal

from datetime import datetime
import socket

from lib import log_nvidia_smi_output, get_srcrepobindirs

srcdir, repodir, bindirs = get_srcrepobindirs()
os.environ['PATH'] = os.pathsep.join([*bindirs, *os.environ['PATH'].split(os.pathsep)])

FLAGS = None

processes = []

def term(signum, frame):
    for p in processes:
        p.terminate()
    sys.exit()

signal.signal(signal.SIGTERM, term)

def main():
  flags = vars(FLAGS)
  for key in sorted(flags.keys()):
    print('%s = %s' % (key, flags[key]))

  if FLAGS.restore_from:
    mode='a'
    start_checkpoint=os.path.join(FLAGS.logdir, "generalize_MODEL", "ckpt-"+FLAGS.restore_from)
  else:
    mode='w'
    start_checkpoint=''

  async def redirect(cmd):
    with open(cmd[-1], 'a') as fid:
      proc = await asyncio.create_subprocess_exec(*cmd[:-1],
                                                  stderr=asyncio.subprocess.PIPE,
                                                  stdout=fid)
      processes.append(proc)
      await proc.communicate()

  async def doit():
    cmds = []
    for isubset,subset in enumerate(FLAGS.subsets):
      model=str(FLAGS.ioffset+isubset+1)+"w"
      expr=["python", os.path.join(srcdir,"loop"),
            "--context="+str(FLAGS.context),
            "--shiftby="+str(FLAGS.shiftby),
            "--optimizer="+FLAGS.optimizer,
            "--loss="+FLAGS.loss,
            "--overlapped_prefix="+FLAGS.overlapped_prefix,
            "--learning_rate="+str(FLAGS.learning_rate),
            "--audio_read_plugin="+FLAGS.audio_read_plugin,
            "--audio_read_plugin_kwargs="+FLAGS.audio_read_plugin_kwargs,
            "--video_read_plugin="+FLAGS.video_read_plugin,
            "--video_read_plugin_kwargs="+FLAGS.video_read_plugin_kwargs,
            "--video_findfile="+FLAGS.video_findfile,
            "--video_bkg_frames="+str(FLAGS.video_bkg_frames),
            "--data_loader_queuesize="+str(FLAGS.data_loader_queuesize),
            "--data_loader_maxprocs="+str(FLAGS.data_loader_maxprocs),
            "--model_architecture="+FLAGS.model_architecture,
            "--model_parameters="+FLAGS.model_parameters.replace('<','^^^<').replace('>','^^^>'),
            "--data_dir="+FLAGS.data_dir,
            "--labels_touse="+FLAGS.labels_touse,
            "--kinds_touse="+FLAGS.kinds_touse,
            "--how_many_training_steps="+str(FLAGS.nsteps),
            "--start_checkpoint="+start_checkpoint.replace("MODEL",model),
            "--save_step_period="+str(FLAGS.save_and_validate_period),
            "--validate_step_period="+str(FLAGS.save_and_validate_period),
            "--batch_size="+str(FLAGS.mini_batch),
            "--testing_files="+FLAGS.testing_files,
            "--time_units="+str(FLAGS.time_units),
            "--freq_units="+str(FLAGS.freq_units),
            "--time_scale="+str(FLAGS.time_scale),
            "--freq_scale="+str(FLAGS.freq_scale),
            "--audio_tic_rate="+str(FLAGS.audio_tic_rate),
            "--audio_nchannels="+str(FLAGS.audio_nchannels),
            "--video_frame_rate="+str(FLAGS.video_frame_rate),
            "--video_frame_width="+str(FLAGS.video_frame_width),
            "--video_frame_height="+str(FLAGS.video_frame_height),
            "--video_channels="+FLAGS.video_channels,
            "--random_seed_batch="+str(FLAGS.batch_seed),
            "--random_seed_weights="+str(FLAGS.weights_seed),
            "--augment_volume="+str(FLAGS.augment_volume),
            "--augment_noise="+str(FLAGS.augment_noise),
            "--augment_dc="+str(FLAGS.augment_dc),
            "--augment_reverse="+str(FLAGS.augment_reverse),
            "--augment_invert="+str(FLAGS.augment_invert),
            "--deterministic="+FLAGS.deterministic,
            "--train_dir="+os.path.join(FLAGS.logdir,"generalize_"+model),
            "--summaries_dir="+os.path.join(FLAGS.logdir,"summaries_"+model),
            "--validation_files="+subset,
            "--validation_percentage=0",
            "--validation_offset_percentage=0",
            "--igpu="+FLAGS.igpu]

            #"--subsample_label=mel-notpulse,mel-pulse,mel-time",
            #"--subsample_skip=2048,2048,256",
            
      cmds.append(expr+[os.path.join(FLAGS.logdir,"generalize_"+model+".log")])
      with open(cmds[-1][-1], mode) as fid:
        fid.write(' '.join(cmds[-1][:-1])+'\n')

    await asyncio.gather(*[redirect(x) for x in cmds])

  asyncio.run(doit())

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--data_dir',
      type=str,
      default='/tmp/speech_dataset/',
      help="""\
      Where to download the speech training data to.
      """)
  parser.add_argument(
      '--shiftby',
      type=float,
      default=100.0,
      help="""\
      Range to shift the training audio by in time.
      """)
  parser.add_argument(
      '--testing_files',
      type=str,
      default='',
      help='Which wav files to use as a test set.')
  parser.add_argument(
      '--time_units',
      type=str,
      default="ms",
      help='Units of time',)
  parser.add_argument(
      '--freq_units',
      type=str,
      default="Hz",
      help='Units of frequency',)
  parser.add_argument(
      '--time_scale',
      type=float,
      default="ms",
      help='This many seconds are in time_units',)
  parser.add_argument(
      '--freq_scale',
      type=float,
      default="Hz",
      help='This many frequencies are in freq_units',)
  parser.add_argument(
      '--audio_tic_rate',
      type=int,
      default=16000,
      help='Expected tic rate of the wavs',)
  parser.add_argument(
      '--audio_nchannels',
      type=int,
      default=1,
      help='Expected number of channels in the wavs',)
  parser.add_argument(
      '--video_frame_rate',
      type=int,
      default=0,
      help='Expected frame rate in Hz of the video',)
  parser.add_argument(
      '--video_frame_width',
      type=int,
      default=0,
      help='Expected frame width in pixels of the video',)
  parser.add_argument(
      '--video_frame_height',
      type=int,
      default=0,
      help='Expected frame height in pixels of the video',)
  parser.add_argument(
      '--video_channels',
      type=str,
      default='1',
      help='Comma-separated list of which channels in the video to use',)
  parser.add_argument(
      '--context',
      type=float,
      default=1000,
      help='Expected duration in milliseconds of the wavs',)
  parser.add_argument(
      '--learning_rate',
      type=float,
      default=0.001,
      help='How large a learning rate to use when training.')
  parser.add_argument(
      '--optimizer',
      type=str,
      default='sgd',
      help='What optimizer to use.  One of Adadelta, Adagrad, Adam, Adamax, Ftrl, Nadam, RMSProp, or SGD.')
  parser.add_argument(
      '--loss',
      type=str,
      default='exclusive',
      choices=['exclusive', 'overlapped'],
      help='Sigmoid cross entropy is used for "overlapped" labels while softmax cross entropy is used for "exclusive" labels.')
  parser.add_argument(
      '--overlapped_prefix',
      type=str,
      default='not_',
      help='When `loss` is `overlapped`, a label starting which this string indicates the absence of the class.  E.g. `song` and `not_song`.')
  parser.add_argument(
      '--nsteps',
      type=int,
      default=15000,
      help='How many training loops to run',)
  parser.add_argument(
      '--save_and_validate_period',
      type=int,
      default=400,
      help='How often to checkpoint and evaluate the training results.')
  parser.add_argument(
      '--mini_batch',
      type=int,
      default=100,
      help='How many items to train with at once',)
  parser.add_argument(
      '--labels_touse',
      type=str,
      default='yes,no,up,down,left,right,on,off,stop,go',
      help='Words to use (others will be added to an unknown label)',)
  parser.add_argument(
      '--kinds_touse',
      type=str,
      default='annotated,classified',
      help='A comma-separted list of "annotated", "detected" , or "classified"',)
  parser.add_argument(
      '--logdir',
      type=str,
      default='/tmp/speech_commands_train',
      help='Directory to write event logs and checkpoint.')
  parser.add_argument(
      '--restore_from',
      type=str,
      default='',
      help='If specified, restore this pretrained model before any training.')
  parser.add_argument(
      '--batch_seed',
      type=int,
      default=59185,
      help='Randomize mini-batch selection if -1; otherwise use supplied number as seed.')
  parser.add_argument(
      '--weights_seed',
      type=int,
      default=59185,
      help='Randomize weight initialization if -1; otherwise use supplied number as seed.')
  parser.add_argument(
      '--augment_volume',
      type=str,
      default='1,1',
      help='Multiply each annotation by a uniform random number in this interval when training')
  parser.add_argument(
      '--augment_noise',
      type=str,
      default='0,0',
      help='Add noise to each annotation with a uniform random std dev in this interval when training')
  parser.add_argument(
      '--augment_dc',
      type=str,
      default='0,0',
      help='Add to each annotation a uniform random number in this interval when training')
  parser.add_argument(
      '--augment_reverse',
      type=str,
      default='no',
      help='Flip in time with a probability of half each annotation when training')
  parser.add_argument(
      '--augment_invert',
      type=str,
      default='no',
      help='Negate with a probability of half each annotation when training')
  parser.add_argument(
      '--model_architecture',
      type=str,
      default='convolutional',
      help='What model architecture to use')
  parser.add_argument(
      '--video_findfile',
      type=str,
      default='same-basename',
      help='What function to use to match WAV files to corresponding video files')
  parser.add_argument(
      '--video_bkg_frames',
      type=int,
      default=1000,
      help='How many frames to use to calculate the median background image')
  parser.add_argument(
      '--audio_read_plugin',
      type=str,
      default="load-wav",
      help='What function to use to read audio files')
  parser.add_argument(
      '--audio_read_plugin_kwargs',
      type=str,
      default="{}",
      help='What default arguments to use to read audio files')
  parser.add_argument(
      '--video_read_plugin',
      type=str,
      default="load-avi-mp4-mov",
      help='What function to use to read video files')
  parser.add_argument(
      '--video_read_plugin_kwargs',
      type=str,
      default="{}",
      help='What default arguments to use to read video files')
  parser.add_argument(
      '--data_loader_queuesize',
      type=int,
      default=0,
      help='How many mini-batches to load in advance')
  parser.add_argument(
      '--data_loader_maxprocs',
      type=int,
      default=0,
      help='The limit of how many extra processes to use to load mini-batches')
  parser.add_argument(
      '--model_parameters',
      type=str,
      default='{}',
      help='What model parameters to use')
  parser.add_argument(
      '--deterministic',
      type=str,
      default='0',
      help='')
  parser.add_argument(
      '--ioffset',
      type=int,
      default='0',
      help='')
  parser.add_argument(
      '--subsets',
      type=str,
      nargs='+',
      help='A whitespace separated list of comma separated files to withhold')
  parser.add_argument(
      '--igpu',
      type=str,
      default='songexplorer_use_all_gpus',
      help='If a comma separated list of numbers, use those GPU(s); if alphanumeric, use the GPUs specified in that environment variable; otherwise, use them all.')

  FLAGS, unparsed = parser.parse_known_args()

  print(str(datetime.now())+": start time")
  with open(os.path.join(repodir, "VERSION.txt"), 'r') as fid:
    print('SongExplorer version = '+fid.read().strip().replace('\n',', '))
  print("hostname = "+socket.gethostname())
  log_nvidia_smi_output(FLAGS.igpu)

  try:
    main()

  except Exception as e:
    print(e)

  finally:
    if hasattr(os, 'sync'):
      os.sync()
    print(str(datetime.now())+": finish time")
