#!/usr/bin/env python3

# plot accuracy across hyperparameter values
 
# e.g. compare <logdirs-prefix>
#     --logdirs_prefix=trained- \
#     --loss=exclusive \
#     --overlapped_prefix=not_

import argparse
import sys
import os
import numpy as np
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
#plt.ion()
import matplotlib.cm as cm
from natsort import natsorted
from functools import reduce
from datetime import datetime
import socket

from lib import *
from jitter import *

srcdir, repodir, _ = get_srcrepobindirs()

FLAGS = None

def main():
    flags = vars(FLAGS)
    for key in sorted(flags.keys()):
        print('%s = %s' % (key, flags[key]))

    logdirs_prefix = FLAGS.logdirs_prefix
    basename, dirname = os.path.split(logdirs_prefix)

    same_time=False
    outlier_criteria=50

    train_time={}
    validation_precision={}
    validation_recall={}
    validation_time={}
    validation_step={}
    labels_touse={}
    nparameters_total={}
    nparameters_finallayer={}
    batch_size={}
    nlayers={}
    hyperparameters={}

    logdirs = list(filter(lambda x: x.startswith(dirname+'-') and \
                          os.path.isdir(os.path.join(basename,x)), os.listdir(basename)))

    for logdir in logdirs:
        print(logdir)
        hyperparameters[logdir] = set(logdir.split('-')[-1].split('_'))
        _, _, train_time[logdir], _, \
                _, _, validation_precision[logdir], validation_recall[logdir], \
                validation_time[logdir], validation_step[logdir], \
                _, _, _, _, \
                labels_touse[logdir], _, \
                nparameters_total[logdir], nparameters_finallayer[logdir], \
                batch_size[logdir], nlayers[logdir] = \
                read_logs(os.path.join(basename,logdir))
        if len(set([tuple(x) for x in labels_touse[logdir].values()]))>1:
            print('WARNING: not all labels_touse are the same')
        if len(set(nparameters_total[logdir].values()))>1:
            print('WARNING: not all nparameters_total are the same')
        if len(set(nparameters_finallayer[logdir].values()))>1:
            print('WARNING: not all nparameters_finallayer are the same')
        if len(set(batch_size[logdir].values()))>1:
            print('WARNING: not all batch_size are the same')
        if len(set(nlayers[logdir].values()))>1:
            print('WARNING: not all nlayers are the same')
        if len(validation_recall)>0:
            if set([tuple(x) for x in labels_touse[logdirs[0]].values()])!=set([tuple(x) for x in labels_touse[logdir].values()]):
                print('WARNING: not all labels_touse are the same')
            if set(nparameters_total[logdirs[0]].values())!=set(nparameters_total[logdir].values()):
                print('WARNING: not all nparameters_total are the same')
            if set(nparameters_finallayer[logdirs[0]].values())!=set(nparameters_finallayer[logdir].values()):
                print('WARNING: not all nparameters_finallayer are the same')
            if set(batch_size[logdirs[0]].values())!=set(batch_size[logdir].values()):
                print('WARNING: not all batch_size are the same')
            if set(nlayers[logdirs[0]].values())!=set(nlayers[logdir].values()):
                print('WARNING: not all nlayers are the same')

    commonparameters = reduce(lambda x,y: x&y, hyperparameters.values())
    differentparameters = {x:','.join(natsorted(list(hyperparameters[x]-commonparameters))) \
                           for x in natsorted(logdirs)}


    fig = plt.figure(figsize=(8,10*2/3))

    ax = fig.add_subplot(2,2,1)

    precisions_mean, recalls_mean = [], []
    for (ilogdir,logdir) in enumerate(natsorted(logdirs)):
        color = cm.viridis(ilogdir/max(1,len(validation_recall)-1))
        precisions_all, recalls_all = [], []
        for model in validation_recall[logdir].keys():
            precisions_all.append(max(validation_precision[logdir][model]))
            recalls_all.append(max(validation_recall[logdir][model]))
            ax.plot(recalls_all[-1], precisions_all[-1], 'o', markeredgecolor='w', color=color)
        precisions_mean.append(np.nanmean(precisions_all))
        recalls_mean.append(np.nanmean(recalls_all))
        ax.plot(recalls_mean[-1], precisions_mean[-1], 'o', markeredgecolor='k', color=color)
    label_precisions_recall(ax, recalls_mean, precisions_mean, "", False)

    ax = fig.add_subplot(2,2,2)
    bottom=100
    for (iexpt,expt) in enumerate(natsorted(validation_recall.keys())):
        color = cm.viridis(iexpt/max(1,len(validation_recall)-1))
        validation_recall_average = np.zeros(len(next(iter(validation_recall[expt].values()))))
        for model in validation_time[expt].keys():
            ax.plot(np.array(validation_time[expt][model])/60,
                    validation_recall[expt][model], \
                    color=color, zorder=iexpt, linewidth=1)
            bottom = min([bottom]+[x for x in validation_recall[expt][model] if x>outlier_criteria])
            validation_recall_average += validation_recall[expt][model]
        line, = ax.plot(np.array(next(iter(validation_time[expt].values())))/60,
                        validation_recall_average / len(validation_time[expt].keys()),
                        color=color, zorder=len(validation_recall)+iexpt, linewidth=3)
        line.set_label(differentparameters[expt])
    ax.set_ylim(bottom=bottom-5, top=100)
    ax.set_xlabel('Training time (min)')
    ax.set_ylabel('Overall validation recall')
    ax.legend(loc='lower right', title=dirname, ncol=2 if "Annotations" in dirname else 1)

    ax = fig.add_subplot(2,2,3)
    ldata = natsorted(nparameters_total.keys())
    xdata = range(len(ldata))
    ydata = [next(iter(nparameters_total[x].values())) - \
             next(iter(nparameters_finallayer[x].values())) for x in ldata]
    ydata2 = [next(iter(nparameters_finallayer[x].values())) for x in ldata]
    bar1 = ax.bar(xdata,ydata,color='k')
    bar2 = ax.bar(xdata,ydata2,bottom=ydata,color='gray')
    ax.legend((bar2,bar1), ('last','rest'))
    ax.set_xlabel(dirname)
    ax.set_ylabel('Trainable parameters')
    ax.set_xticks(xdata)
    ax.set_xticklabels([differentparameters[x] for x in ldata], rotation=40, ha='right')

    ax = fig.add_subplot(2,2,4)
    data = {k:list([np.median(np.diff(x)) for x in train_time[k].values()]) for k in train_time}
    ldata = jitter_plot(ax, data)
    ax.set_ylabel('time / step (ms)')
    ax.set_xlabel(dirname)
    ax.set_xticks(range(len(ldata)))
    ax.set_xticklabels([differentparameters[x] for x in ldata], rotation=40, ha='right')

    fig.suptitle(','.join(list(commonparameters)))

    fig.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(logdirs_prefix+'-compare-overall-params-speed.pdf')
    plt.close()


    recall_confusion_matrices={}
    precision_confusion_matrices={}
    labels=None

    for ilogdir,logdir in enumerate(natsorted(logdirs)):
        kind = next(iter(validation_time[logdir].keys())).split('_')[0]
        confusion_matrices, theselabels = \
                parse_confusion_matrices(os.path.join(basename,logdir), kind, \
                                         idx_time=idx_time[logdir] if same_time else None)

        recall_confusion_matrices[logdir]={}
        precision_confusion_matrices[logdir]={}
        for model in confusion_matrices.keys():
            best_F1=("",0)
            for ckpt in confusion_matrices[model].keys():
                _precision_confusion_matrix, _recall_confusion_matrix, _precisions_mean, _recalls_mean = \
                      normalize_confusion_matrix(np.sum(confusion_matrices[model][ckpt], axis=0))
                P,R = _precisions_mean, _recalls_mean
                if 2*P*R/(P+R) > best_F1[1]:
                    best_F1 = (ckpt, 2*P*R/(P+R))
                    precision_confusion_matrices[logdir][model] = _precision_confusion_matrix
                    recall_confusion_matrices[logdir][model] = _recall_confusion_matrix
            ckpt = best_F1[0]
            confusion_matrices[model] = np.array(confusion_matrices[model][ckpt])

        if not labels:
            labels=theselabels
            if FLAGS.loss=='exclusive':
                scale=6.4
                nrows, ncols = layout(len(logdirs))
            else:
                scale=4.8
                nrows, ncols = 1+len(labels), len(logdirs)
            fig = plt.figure(figsize=(scale*ncols, scale*3/4*nrows))
        assert set(labels)==set(theselabels)
        if labels!=theselabels:
            idx = [labels.index(x) for x in theselabels]
            confusion_matrices = {k: [[confusion_matrices[k][i][j] for j in idx] \
                                      for i in idx] \
                                  for k in confusion_matrices.keys()}

        summed_confusion_matrix = None
        for model in confusion_matrices.keys():
            if summed_confusion_matrix is None:
                summed_confusion_matrix = np.array(confusion_matrices[model])
            else:
                summed_confusion_matrix += confusion_matrices[model]
        summed2_confusion_matrix = np.sum(summed_confusion_matrix, axis=0)
    
        precision_summed_matrix, recall_summed_matrix, precision_summed, recall_summed = \
              normalize_confusion_matrix(summed2_confusion_matrix)
    
        ax = fig.add_subplot(nrows, ncols, 1+ilogdir)
        plot_confusion_matrix(fig, ax,
                              summed2_confusion_matrix,
                              precision_summed_matrix, recall_summed_matrix,
                              len(labels)<10,
                              logdir+"\n",
                              labels if FLAGS.loss=='exclusive' else
                                  ["song", FLAGS.overlapped_prefix+"song"],
                              precision_summed, recall_summed)

        if FLAGS.loss=='overlapped':
            for ilabel in range(len(labels)):
                ax = plt.subplot(nrows, ncols, ilogdir+1+ncols*(1+ilabel))
                precision_summed_matrix, recall_summed_matrix, precision_summed, recall_summed = \
                      normalize_confusion_matrix(summed_confusion_matrix[ilabel])
                plot_confusion_matrix(fig, ax,
                                      summed_confusion_matrix[ilabel], \
                                      precision_summed_matrix, recall_summed_matrix, \
                                      len(labels)<10,
                                      logdir+"\n",
                                      [labels[ilabel], FLAGS.overlapped_prefix+labels[ilabel]],
                                      precision_summed, recall_summed)

    fig.tight_layout()
    plt.savefig(logdirs_prefix+'-compare-confusion-matrices.pdf')
    plt.close()


    nrows, ncols = layout(len(labels))
    scale=4.8
    fig = plt.figure(figsize=(scale*ncols, scale*nrows))

    for (ilabel,label) in enumerate(labels):
        ax = fig.add_subplot(nrows, ncols, ilabel+1)
        precisions_mean, recalls_mean = [], []
        for (ilogdir,logdir) in enumerate(natsorted(logdirs)):
            color = cm.viridis(ilogdir/max(1,len(validation_recall)-1))
            precisions_all, recalls_all = [], []
            for (imodel,model) in enumerate(recall_confusion_matrices[logdir].keys()):
                precisions_all.append(100*precision_confusion_matrices[logdir][model][ilabel][ilabel])
                recalls_all.append(100*recall_confusion_matrices[logdir][model][ilabel][ilabel])
                line, = ax.plot(recalls_all[-1], precisions_all[-1],
                                'o', markeredgecolor='w', color=color)
                if imodel==0:
                    line.set_label(differentparameters[logdir])
            precisions_mean.append(np.nanmean(precisions_all))
            recalls_mean.append(np.nanmean(recalls_all))
            ax.plot(recalls_mean[-1], precisions_mean[-1],
                    'o', markeredgecolor='k', color=color)
        label_precisions_recall(ax, recalls_mean, precisions_mean, label+"\n")

    fig.tight_layout()
    plt.savefig(logdirs_prefix+'-compare-PR-classes.pdf')
    plt.close()
  
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--logdirs_prefix',
        type=str,
        default='/tmp/speech_commands_train',
        help='Common prefix of the directories of logs and checkpoints')
    parser.add_argument(
        '--loss',
        type=str,
        default='exclusive',
        choices=['exclusive', 'overlapped'],
        help='Sigmoid cross entropy is used for "overlapped" labels while softmax cross entropy is used for "exclusive" labels.')
    parser.add_argument(
        '--overlapped_prefix',
        type=str,
        default='not_',
        help='When `loss` is `overlapped`, a label starting which this string indicates the absence of the class.  E.g. `song` and `not_song`.')

    FLAGS, unparsed = parser.parse_known_args()

    print(str(datetime.now())+": start time")
    with open(os.path.join(repodir, "VERSION.txt"), 'r') as fid:
        print('SongExplorer version = '+fid.read().strip().replace('\n',', '))
    print("hostname = "+socket.gethostname())

    try:
        main()
    
    except Exception as e:
        print(e)
    
    finally:
        if hasattr(os, 'sync'):
            os.sync()
        print(str(datetime.now())+": finish time")
