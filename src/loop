#!/usr/bin/env python

#This file, originally from the TensorFlow speech recognition tutorial,
#has been heavily modified for use by SongExplorer.


# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
r"""Simple speech recognition to spot a limited number of keywords.

This is a self-contained example script that will train a very basic audio
recognition model in TensorFlow. It downloads the necessary training data and
runs with reasonable defaults to train within a few hours even only using a CPU.
For more information, please see
https://www.tensorflow.org/tutorials/audio_recognition.

It is intended as an introduction to using neural networks for audio
recognition, and is not a full speech recognition system. For more advanced
speech systems, I recommend looking into Kaldi. This network uses a keyword
detection style to spot discrete words from a small vocabulary, consisting of
"yes", "no", "up", "down", "left", "right", "on", "off", "stop", and "go".

To run the training process, use:

bazel run tensorflow/examples/speech_commands:train

This will write out checkpoints to /tmp/speech_commands_train/, and will
download over 1GB of open source training data, so you'll need enough free space
and a good internet connection. The default data is a collection of thousands of
one-second .wav files, each containing one spoken word. This data set is
collected from https://aiyprojects.withgoogle.com/open_speech_recording, please
consider contributing to help improve this and other models!

As training progresses, it will print out its accuracy metrics, which should
rise above 90% by the end. Once it's complete, you can run the freeze script to
get a binary GraphDef that you can easily deploy on mobile applications.

If you want to train on your own data, you'll need to create .wavs with your
recordings, all at a consistent length, and then arrange them into subfolders
organized by label. For example, here's a possible file structure:

my_wavs >
  up >
    audio_0.wav
    audio_1.wav
  down >
    audio_2.wav
    audio_3.wav
  other>
    audio_4.wav
    audio_5.wav

You'll also need to tell the script what labels to look for, using the
`--labels_touse` argument. In this case, 'up,down' might be what you want, and
the audio in the 'other' folder would be used to train an 'unknown' category.

To pull this all together, you'd run:

bazel run tensorflow/examples/speech_commands:train -- \
--data_dir=my_wavs --labels_touse=up,down

"""
import argparse
import os
import sys
import re
import gc

import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import *

import data as D

from datetime import datetime
import socket

import json

import importlib

from lib import get_srcrepobindirs, add_plugins_to_path, log_nvidia_smi_output, select_GPUs

FLAGS = None

# Create the back propagation and training evaluation machinery

def masked_sigmoid_cross_entropy_with_logits(labels, logits):
    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels, logits)
    mask = tf.math.not_equal(labels, 2)
    return loss * tf.cast(mask, tf.float32)

def mean_square_error(labels, logits):
    return tf.keras.losses.MSE(labels, logits)

def loss_fn(fn, logits, ground_truth_input):
    if FLAGS.loss != 'autoencoder':
        return fn(labels=ground_truth_input, logits=logits[:,0,:])
    else:
        return fn(labels=ground_truth_input, logits=logits)

def evaluate_fn(loss, model, fingerprint_input, ground_truth_input, istraining):
    hidden_activations, logits = model(fingerprint_input, training=istraining)
    if loss == 'exclusive':
        predicted_indices = tf.cast(tf.math.argmax(logits, 2)[:,0], tf.int32)
        correct_predictions = tf.equal(predicted_indices, ground_truth_input)
        mask = None
    elif loss == 'overlapped':
        predicted_indices = tf.cast(logits > 0.5, tf.float32)[:,0,:]
        all_predictions = tf.equal(predicted_indices, ground_truth_input)
        mask = tf.math.not_equal(ground_truth_input, 2)
        correct_predictions = tf.boolean_mask(all_predictions, mask)
    elif loss == 'autoencoder':
        correct_predictions = tf.math.abs(logits - ground_truth_input)
        predicted_indices = mask = None
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))
    return logits, accuracy, predicted_indices, hidden_activations, mask

def main():
  os.environ['TF_DETERMINISTIC_OPS']=FLAGS.deterministic
  os.environ['TF_DISABLE_SPARSE_SOFTMAX_XENT_WITH_LOGITS_OP_DETERMINISM_EXCEPTIONS']=FLAGS.deterministic

  np.set_printoptions(linewidth=np.inf)

  select_GPUs(FLAGS.igpu)

  srcdir, repodir, bindirs = get_srcrepobindirs()
  add_plugins_to_path(srcdir)

  sys.path.append(os.path.dirname(FLAGS.model_architecture))
  model = importlib.import_module(os.path.basename(FLAGS.model_architecture))

  sys.path.append(os.path.dirname(FLAGS.video_findfile))
  video_findfile = importlib.import_module(os.path.basename(FLAGS.video_findfile)).video_findfile

  flags = vars(FLAGS)
  for key in sorted(flags.keys()):
    print('%s = %s' % (key, flags[key]))

  if FLAGS.random_seed_weights!=-1:
      tf.keras.utils.set_random_seed(FLAGS.random_seed_weights)

  for physical_device in tf.config.experimental.list_physical_devices('GPU'):
    tf.config.experimental.set_memory_growth(physical_device, True)
  tf.config.set_soft_device_placement(True)

  if FLAGS.start_checkpoint:
    labels_touse = tf.io.gfile.GFile(os.path.join(FLAGS.train_dir, 'labels.txt')).readlines()
    labels_touse = ','.join([x.rstrip() for x in labels_touse])
    if labels_touse != FLAGS.labels_touse:
      if set(labels_touse.split(',')) != set(FLAGS.labels_touse.split(',')):
        print('ERROR: labels_touse does not match model being restored from')
        return
      else:
        print('WARNING: labels_touse is out of order.  continuing with order from restored model')
  else:
    labels_touse = FLAGS.labels_touse

  nlabels = len(labels_touse.split(','))

  model_settings = {'nlabels': nlabels,
                    'time_units': FLAGS.time_units,
                    'freq_units': FLAGS.freq_units,
                    'time_scale': FLAGS.time_scale,
                    'freq_scale': FLAGS.freq_scale,
                    'audio_tic_rate': FLAGS.audio_tic_rate,
                    'audio_nchannels': FLAGS.audio_nchannels,
                    'video_frame_rate': FLAGS.video_frame_rate,
                    'video_frame_width': FLAGS.video_frame_width,
                    'video_frame_height': FLAGS.video_frame_height,
                    'video_channels': [int(x)-1 for x in FLAGS.video_channels.split(',')],
                    'augment_volume': FLAGS.augment_volume,
                    'augment_noise': FLAGS.augment_noise,
                    'augment_dc': FLAGS.augment_dc,
                    'augment_reverse': FLAGS.augment_reverse,
                    'augment_invert': FLAGS.augment_invert,
                    'parallelize': 1,
                    'context': FLAGS.context}

  D.init(
      FLAGS.data_dir,
      FLAGS.shiftby,
      labels_touse.split(','), FLAGS.kinds_touse.split(','),
      FLAGS.validation_percentage, FLAGS.validation_offset_percentage,
      FLAGS.validation_files.split(','),
      0, FLAGS.testing_files.split(','), FLAGS.subsample_skip,
      FLAGS.subsample_label,
      FLAGS.partition_label, FLAGS.partition_n, FLAGS.partition_training_files.split(','),
      FLAGS.partition_validation_files.split(','),
      FLAGS.random_seed_batch,
      FLAGS.testing_equalize_ratio, FLAGS.testing_max_sounds,
      model_settings, FLAGS.loss, FLAGS.overlapped_prefix,
      FLAGS.data_loader_queuesize, FLAGS.data_loader_maxprocs,
      model.use_audio, model.use_video, video_findfile, FLAGS.video_bkg_frames,
      FLAGS.audio_read_plugin, FLAGS.video_read_plugin,
      FLAGS.audio_read_plugin_kwargs, FLAGS.video_read_plugin_kwargs)

  thismodel = model.create_model(model_settings, FLAGS.model_parameters)
  thismodel.summary(line_length=120, positions=[0.4,0.6,0.7,1])

  thisoptimizer = getattr(tf.keras.optimizers, FLAGS.optimizer)(learning_rate=FLAGS.learning_rate)

  if FLAGS.loss=='exclusive':
      thisloss = tf.nn.sparse_softmax_cross_entropy_with_logits
  elif FLAGS.loss=='overlapped':
      thisloss = masked_sigmoid_cross_entropy_with_logits
  elif FLAGS.loss=='autoencoder':
      thisloss = mean_square_error

  start_step = 1

  checkpoint = tf.train.Checkpoint(thismodel=thismodel, thisoptimizer=thisoptimizer)
  checkpoint_basepath = os.path.join(FLAGS.train_dir, 'ckpt')

  if FLAGS.start_checkpoint:
    checkpoint.read(FLAGS.start_checkpoint)
    start_step = 1 + int(os.path.basename(FLAGS.start_checkpoint).split('-')[-1])
  else:
    print('Saving to "%s-%d"' % (checkpoint_basepath, 0))
    checkpoint.write(checkpoint_basepath+'-0')

  t0 = datetime.now()
  print('Training from time %s, step: %d ' % (t0.isoformat(), start_step))

  if not FLAGS.start_checkpoint:
    with tf.io.gfile.GFile(os.path.join(FLAGS.train_dir, 'labels.txt'), 'w') as f:
      f.write(labels_touse.replace(',','\n'))

  train_writer = tf.summary.create_file_writer(FLAGS.summaries_dir + '/train')
  validation_writer = tf.summary.create_file_writer(FLAGS.summaries_dir + '/validation')

  # exit if how_many_training_steps==0
  if FLAGS.how_many_training_steps==0:
      # pre-process a batch of data to make sure settings are valid
      train_fingerprints, train_ground_truth, _ = D.get_data(
          FLAGS.batch_size, 0, model_settings, FLAGS.loss,
          FLAGS.overlapped_prefix, FLAGS.shiftby, 'training',
          model.use_audio, model.use_video, video_findfile)
      evaluate_fn(FLAGS.loss, thismodel, train_fingerprints, train_ground_truth, False)
      return

  training_set_size = D.set_size('training')
  testing_set_size = D.set_size('testing')
  validation_set_size = D.set_size('validation')

  @tf.function
  def train_step(train_fingerprints, train_ground_truth):
    # Run the graph with this batch of training data.
    with tf.GradientTape() as tape:
      logits, train_accuracy, _, _, _ = evaluate_fn(FLAGS.loss, thismodel, train_fingerprints,
                                                    train_ground_truth, True)
      loss_value = loss_fn(thisloss, logits, train_ground_truth)
    gradients = tape.gradient(loss_value, thismodel.trainable_variables)
    thisoptimizer.apply_gradients(zip(gradients, thismodel.trainable_variables))
    cross_entropy_mean = tf.math.reduce_mean(loss_value)
    return cross_entropy_mean, train_accuracy

  # Training loop.
  if FLAGS.loss != 'autoencoder':
    print('line format is Elapsed %f, Step #%d, Train accuracy %.1f, cross entropy %f')
  else:
    print('line format is Elapsed %f, Step #%d, Train cross entropy %f')
  for training_step in range(start_step, FLAGS.how_many_training_steps + 1):
    if training_set_size>0:
      # Pull the sounds we'll use for training.
      train_fingerprints, train_ground_truth, _ = D.get_data(
          FLAGS.batch_size, 0, model_settings, FLAGS.loss,
          FLAGS.overlapped_prefix, FLAGS.shiftby, 'training',
          model.use_audio, model.use_video, video_findfile)

      cross_entropy_mean, train_accuracy = train_step(train_fingerprints, train_ground_truth)
      t1=datetime.now()-t0

      with train_writer.as_default():
        tf.summary.scalar('cross_entropy', cross_entropy_mean, step=training_step)
        tf.summary.scalar('accuracy', train_accuracy, step=training_step)

      if FLAGS.loss != 'autoencoder':
        print('%f,%d,%.1f,%f' %
                        (t1.total_seconds(), training_step,
                         train_accuracy.numpy() * 100, cross_entropy_mean.numpy()))
      else:
        print('%f,%d,%f' % (t1.total_seconds(), training_step, cross_entropy_mean.numpy()))

      # Save the model checkpoint periodically.
      if ((FLAGS.save_step_period > 0 and training_step % FLAGS.save_step_period == 0) or
          training_step == FLAGS.how_many_training_steps):
        print('Saving to "%s-%d"' % (checkpoint_basepath, training_step))
        checkpoint.write(checkpoint_basepath+'-'+str(training_step))

    if validation_set_size>0 and \
       training_step != FLAGS.how_many_training_steps and \
       (FLAGS.validate_step_period > 0 and training_step % FLAGS.validate_step_period == 0):
      gc.collect()
      validate_and_test(thismodel, thisloss, 'validation', validation_set_size, model_settings, \
                        False, training_step, t0, nlabels, \
                        validation_writer, model.use_audio, model.use_video,
                        video_findfile, FLAGS.loss)

  if validation_set_size>0:
    validate_and_test(thismodel, thisloss, 'validation', validation_set_size, model_settings, \
                      True, FLAGS.how_many_training_steps, t0, nlabels, \
                      validation_writer, model.use_audio, model.use_video,
                      video_findfile, FLAGS.loss)
  if testing_set_size>0:
    validate_and_test(thismodel, thisloss, 'testing', testing_set_size, model_settings, \
                      True, FLAGS.how_many_training_steps, t0, nlabels, \
                      validation_writer, model.use_audio, model.use_video,
                      video_findfile, FLAGS.loss)

@tf.function
def validate_test_step(model, loss, nlabels, fingerprints, ground_truth, loss_str):
  logits, accuracy, predicted_indices, hidden_activations, mask = evaluate_fn(FLAGS.loss, model,
          fingerprints, ground_truth, False)
  loss_value = loss_fn(loss, logits, ground_truth)
  cross_entropy_mean = tf.math.reduce_mean(loss_value)
  if loss_str == 'exclusive':
      confusion_matrix = tf.math.confusion_matrix(ground_truth,
                                                  predicted_indices,
                                                  num_classes=nlabels)
  elif loss_str == 'overlapped':
    confusion_matrices = []
    for ilabel in range(nlabels):
        confusion_matrices.append(tf.math.confusion_matrix(
                tf.boolean_mask(1-ground_truth[:,ilabel], mask[:,ilabel]),
                tf.boolean_mask(1-predicted_indices[:,ilabel], mask[:,ilabel]),
                num_classes=2))
    confusion_matrix = tf.stack(confusion_matrices)
  elif loss_str == 'autoencoder':
    confusion_matrix = None
  return logits, accuracy, hidden_activations, cross_entropy_mean, confusion_matrix

def validate_and_test(model, loss, set_kind, set_size, model_settings, \
                      is_last_step, training_step, t0, nlabels, \
                      validation_writer, use_audio, use_video, video_findfile,
                      loss_str):
  total_accuracy = 0
  total_conf_matrix = None
  isound = 0
  for _ in range(0, set_size, FLAGS.batch_size):
    # HACK: get_data not guaranteed to return isounds in order
    fingerprints, ground_truth, sounds = (
        D.get_data(FLAGS.batch_size, isound, model_settings,
                                 FLAGS.loss, FLAGS.overlapped_prefix,
                                 FLAGS.shiftby, set_kind,
                                 use_audio, use_video, video_findfile))
    logits, accuracy, hidden_activations, cross_entropy_mean, confusion_matrix = \
            validate_test_step(model, loss, nlabels, fingerprints, ground_truth, loss_str)

    with validation_writer.as_default():
      tf.summary.scalar('cross_entropy', cross_entropy_mean, step=training_step)
      tf.summary.scalar('accuracy', accuracy, step=training_step)

    this_batch_size = fingerprints[0].shape[0] if use_audio and use_video else fingerprints.shape[0]
    total_accuracy += (accuracy * this_batch_size) / set_size
    if loss_str != 'autoencoder':
      if total_conf_matrix is None:
        total_conf_matrix = confusion_matrix
      else:
        total_conf_matrix += confusion_matrix
    if isound==0:
      sounds_data = [None]*set_size
      groundtruth_data = np.empty((set_size, *(np.shape(ground_truth)[1:])))
      if loss_str == 'autoencoder':
          logit_data = np.empty((set_size, *(np.shape(logits)[1:])))
      else:
          logit_data = np.empty((set_size,np.shape(logits)[2]))
    sounds_data[isound:isound+this_batch_size] = sounds
    groundtruth_data[isound:isound+this_batch_size] = ground_truth
    if loss_str == 'autoencoder':
        logit_data[isound:isound+this_batch_size,:] = logits
    else:
        logit_data[isound:isound+this_batch_size,:] = logits[:,0,:]
    if is_last_step:
      if FLAGS.save_hidden:
        if isound==0:
          hidden_layers = []
          for ihidden in range(len(hidden_activations)):
            nHWC = np.shape(hidden_activations[ihidden])[1:]
            hidden_layers.append(np.empty((set_size, *nHWC)))
        for ihidden in range(len(hidden_activations)):
          hidden_layers[ihidden][isound:isound+this_batch_size,...] = hidden_activations[ihidden]
      if FLAGS.save_fingerprints:
        if isound==0:
          if use_audio + use_video == 1:
            nHWC = np.shape(fingerprints)[1:]
            input_layer = np.empty((set_size ,*nHWC))
          else:
            input_layer = []
            nHWC = np.shape(fingerprints[0])[1:]
            input_layer[0] = np.empty((set_size ,*nHWC))
            nHWC = np.shape(fingerprints[1])[1:]
            input_layer[1] = np.empty((set_size ,*nHWC))
        if use_audio + use_video == 1:
          input_layer[isound:isound+this_batch_size,...] = fingerprints
        else:
          input_layer[0][isound:isound+this_batch_size,...] = fingerprints[0]
          input_layer[1][isound:isound+this_batch_size,...] = fingerprints[1]
    isound += this_batch_size
  if loss_str != 'autoencoder':
    print('Confusion Matrix:\n %s\n %s' % \
                    (D.labels_list, total_conf_matrix.numpy()))
    t1=datetime.now()-t0
    print('%f,%d,%.1f %s' %
                    (t1.total_seconds(), training_step, \
                     total_accuracy * 100, set_kind.capitalize()))
  else:
    t1=datetime.now()-t0
    print('%f,%d,%f %s' %
                    (t1.total_seconds(), training_step, \
                     cross_entropy_mean.numpy(), set_kind.capitalize()))
  np.savez(os.path.join(FLAGS.train_dir,
                        'logits.'+set_kind+'.ckpt-'+str(training_step)+'.npz'), \
           sounds=np.asarray(sounds_data, dtype="object"),
           groundtruth=groundtruth_data,
           logits=logit_data)
  if is_last_step:
    if FLAGS.save_hidden:
      np.savez(os.path.join(FLAGS.train_dir,
                            'hidden.'+set_kind+'.ckpt-'+str(training_step)+'.npz'), \
               *hidden_layers, sounds=np.asarray(sounds_data, dtype="object"))
    if FLAGS.save_fingerprints:
      np.savez(os.path.join(FLAGS.train_dir,
                            'fingerprints.'+set_kind+'.ckpt-'+str(training_step)+'.npz'), \
              input_layer, sounds=np.asarray(sounds_data, dtype="object"))

def str2bool(v):
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--data_dir',
      type=str,
      default='/tmp/speech_dataset/',
      help="""\
      Where to download the speech training data to.
      """)
  parser.add_argument(
      '--shiftby',
      type=float,
      default=100.0,
      help="""\
      Range to shift the training audio by in time.
      """)
  parser.add_argument(
      '--testing_files',
      type=str,
      default='',
      help='Which wav files to use as a test set.')
  parser.add_argument(
      '--subsample_label',
      type=str,
      default='',
      help='Train on only a subset of annotations for this label.')
  parser.add_argument(
      '--subsample_skip',
      type=str,
      default='',
      help='Take only every Nth annotation for the specified label.')
  parser.add_argument(
      '--partition_label',
      type=str,
      default='',
      help='Train on only a fixed number of annotations for this label.')
  parser.add_argument(
      '--partition_n',
      type=int,
      default=0,
      help='Train on only this number of annotations from each file for the specified label.')
  parser.add_argument(
      '--partition_training_files',
      type=str,
      default='',
      help='Train on only these files for the specified label.')
  parser.add_argument(
      '--partition_validation_files',
      type=str,
      default='',
      help='Validate on only these files for the specified label.')
  parser.add_argument(
      '--validation_files',
      type=str,
      default='',
      help='Which wav files to use as a validation set.')
  parser.add_argument(
      '--validation_percentage',
      type=float,
      default=10,
      help='What percentage of wavs to use as a validation set.')
  parser.add_argument(
      '--validation_offset_percentage',
      type=float,
      default=0,
      help='Which wavs to use as a cross-validation set.')
  parser.add_argument(
      '--time_units',
      type=str,
      default="ms",
      help='Units of time',)
  parser.add_argument(
      '--freq_units',
      type=str,
      default="Hz",
      help='Units of frequency',)
  parser.add_argument(
      '--time_scale',
      type=float,
      default="ms",
      help='This many seconds are in time_units',)
  parser.add_argument(
      '--freq_scale',
      type=float,
      default="Hz",
      help='This many frequencies are in freq_units',)
  parser.add_argument(
      '--audio_tic_rate',
      type=int,
      default=16000,
      help='Expected tic rate of the wavs',)
  parser.add_argument(
      '--audio_nchannels',
      type=int,
      default=1,
      help='Expected number of channels in the wavs',)
  parser.add_argument(
      '--video_frame_rate',
      type=int,
      default=0,
      help='Expected frame rate in Hz of the video',)
  parser.add_argument(
      '--video_frame_width',
      type=int,
      default=0,
      help='Expected frame width in pixels of the video',)
  parser.add_argument(
      '--video_frame_height',
      type=int,
      default=0,
      help='Expected frame height in pixels of the video',)
  parser.add_argument(
      '--video_channels',
      type=str,
      default='1',
      help='Comma-separated list of which channels in the video to use',)
  parser.add_argument(
      '--context',
      type=float,
      default=1000,
      help='Expected duration in milliseconds of the wavs',)
  parser.add_argument(
      '--learning_rate',
      type=float,
      default=0.001,
      help='How large a learning rate to use when training.')
  parser.add_argument(
      '--optimizer',
      type=str,
      default='sgd',
      help='What optimizer to use.  One of Adadelta, Adagrad, Adam, Adamax, Ftrl, Nadam, RMSProp, or SGD.')
  parser.add_argument(
      '--loss',
      type=str,
      default='exclusive',
      choices=['exclusive', 'overlapped', 'autoencoder'],
      help='Sigmoid cross entropy is used for "overlapped" and "autoencoder" labels while softmax cross entropy is used for "exclusive" labels.')
  parser.add_argument(
      '--overlapped_prefix',
      type=str,
      default='not_',
      help='When `loss` is `overlapped`, a label starting which this string indicates the absence of the class.  E.g. `song` and `not_song`.')
  parser.add_argument(
      '--how_many_training_steps',
      type=int,
      default=15000,
      help='How many training loops to run',)
  parser.add_argument(
      '--validate_step_period',
      type=int,
      default=400,
      help='How often to evaluate the training results.')
  parser.add_argument(
      '--batch_size',
      type=int,
      default=100,
      help='How many items to train with at once',)
  parser.add_argument(
      '--summaries_dir',
      type=str,
      default='/tmp/retrain_logs',
      help='Where to save summary logs for TensorBoard.')
  parser.add_argument(
      '--labels_touse',
      type=str,
      default='yes,no,up,down,left,right,on,off,stop,go',
      help='Words to use (others will be added to an unknown label)',)
  parser.add_argument(
      '--kinds_touse',
      type=str,
      default='annotated,classified',
      help='A comma-separted list of "annotated", "detected" , or "classified"',)
  parser.add_argument(
      '--train_dir',
      type=str,
      default='/tmp/speech_commands_train',
      help='Directory to write event logs and checkpoint.')
  parser.add_argument(
      '--save_step_period',
      type=int,
      default=100,
      help='Save model checkpoint every save_steps.')
  parser.add_argument(
      '--start_checkpoint',
      type=str,
      default='',
      help='If specified, restore this pretrained model before any training.')
  parser.add_argument(
      '--random_seed_batch',
      type=int,
      default=59185,
      help='Randomize mini-batch selection if -1; otherwise use supplied number as seed.')
  parser.add_argument(
      '--random_seed_weights',
      type=int,
      default=59185,
      help='Randomize weight initialization if -1; otherwise use supplied number as seed.')
  parser.add_argument(
      '--augment_volume',
      type=str,
      default='1,1',
      help='Multiply each annotation by a uniform random number in this interval when training')
  parser.add_argument(
      '--augment_noise',
      type=str,
      default='0,0',
      help='Add noise to each annotation with a uniform random std dev in this interval when training')
  parser.add_argument(
      '--augment_dc',
      type=str,
      default='0,0',
      help='Add to each annotation a uniform random number in this interval when training')
  parser.add_argument(
      '--augment_reverse',
      type=str,
      default='no',
      help='Flip in time with a probability of half each annotation when training')
  parser.add_argument(
      '--augment_invert',
      type=str,
      default='no',
      help='Negate with a probability of half each annotation when training')
  parser.add_argument(
      '--testing_equalize_ratio',
      type=int,
      default=0,
      help='Limit most common label to be no more than this times more than the least common label for testing.')
  parser.add_argument(
      '--testing_max_sounds',
      type=int,
      default=0,
      help='Limit number of test sounds to this number.')
  parser.add_argument(
      '--model_architecture',
      type=str,
      default='convolutional',
      help='What model architecture to use')
  parser.add_argument(
      '--video_findfile',
      type=str,
      default='same-basename',
      help='What function to use to match WAV files to corresponding video files')
  parser.add_argument(
      '--video_bkg_frames',
      type=int,
      default=1000,
      help='How many frames to use to calculate the median background image')
  parser.add_argument(
      '--audio_read_plugin',
      type=str,
      default="load-wav",
      help='What function to use to read audio files')
  parser.add_argument(
      '--audio_read_plugin_kwargs',
      type=json.loads,
      default="{}",
      help='What default arguments to use to read audio files')
  parser.add_argument(
      '--video_read_plugin',
      type=str,
      default="load-avi-mp4-mov",
      help='What function to use to read video files')
  parser.add_argument(
      '--video_read_plugin_kwargs',
      type=json.loads,
      default="{}",
      help='What default arguments to use to read video files')
  parser.add_argument(
      '--data_loader_queuesize',
      type=int,
      default=0,
      help='How many mini-batches to load in advance')
  parser.add_argument(
      '--data_loader_maxprocs',
      type=int,
      default=0,
      help='The limit of how many extra processes to use to load mini-batches')
  parser.add_argument(
      '--model_parameters',
      type=json.loads,
      default='{}',
      help='What model parameters to use')
  parser.add_argument(
      '--save_hidden',
      type=str2bool,
      default=False,
      help='Whether to save hidden layer activations during processing')
  parser.add_argument(
      '--save_fingerprints',
      type=str2bool,
      default=False,
      help='Whether to save fingerprint input layer during processing')
  parser.add_argument(
      '--deterministic',
      type=str,
      default='0',
      help='')
  parser.add_argument(
      '--igpu',
      type=str,
      default='songexplorer_use_all_gpus',
      help='If a comma separated list of numbers, use those GPU(s); if alphanumeric, use the GPUs specified in that environment variable; otherwise, use them all.')

  FLAGS, unparsed = parser.parse_known_args()

  print(str(datetime.now())+": start time")
  repodir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
  with open(os.path.join(repodir, "VERSION.txt"), 'r') as fid:
    print('SongExplorer version = '+fid.read().strip().replace('\n',', '))
  print("hostname = "+socket.gethostname())
  log_nvidia_smi_output(FLAGS.igpu)

  os.environ['COLUMNS']='120'

  try:
    main()

  except Exception as e:
    print(e)

  finally:
    if hasattr(os, 'sync'):
      os.sync()
    print(str(datetime.now())+": finish time")
